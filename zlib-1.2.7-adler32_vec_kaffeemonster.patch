Generated by cloneing https://github.com/kaffeemonster/zlib/commits/adler32_vec and merging v1.2.7 into it.

diff --git a/INDEX b/INDEX
index 2ba0641..2e972ed 100644
--- a/INDEX
+++ b/INDEX
@@ -20,16 +20,24 @@ zlib.pc.in      Template for pkg-config descriptor
 zlib.pc.cmakein zlib.pc template for cmake
 zlib2ansi       perl script to convert source files for C++ compilation
 
+alpha/          files for the alpha platform
 amiga/          makefiles for Amiga SAS C
+arm/            files for the arm platform
 as400/          makefiles for AS/400
+bfin/           files for the bfin platform
 doc/            documentation for formats and algorithms
+mips/           files for the mips platform
+ia64/           files for the ia64 platform
 msdos/          makefiles for MSDOS
 nintendods/     makefile for Nintendo DS
 old/            makefiles for various architectures and zlib documentation
                 files that have not yet been updated for zlib 1.2.x
+ppc/            files for the ppc platform
 qnx/            makefiles for QNX
+sparc/          files for the sparc platform
 watcom/         makefiles for OpenWatcom
 win32/          makefiles for Windows
+x86/            files for the x86 platform
 
                 zlib public header files (required for library use):
 zconf.h
diff --git a/Makefile.in b/Makefile.in
index 241deed..d6c02dd 100644
--- a/Makefile.in
+++ b/Makefile.in
@@ -267,7 +267,8 @@ depend:
 
 # DO NOT DELETE THIS LINE -- make depend depends on it.
 
-adler32.o zutil.o: zutil.h zlib.h zconf.h
+adler32.o: adler32.c ppc/adler32.c arm/adler32.c x86/adler32.c alpha/adler32.c mips/adler32.c bfin/adler32.c ia64/adler32.c sparc/adler32.c zutil.h zlib.h zconf.h
+zutil.o: zutil.h zlib.h zconf.h
 gzclose.o gzlib.o gzread.o gzwrite.o: zlib.h zconf.h gzguts.h
 compress.o example.o minigzip.o uncompr.o: zlib.h zconf.h
 crc32.o: zutil.h zlib.h zconf.h crc32.h
@@ -277,7 +278,8 @@ inffast.o: zutil.h zlib.h zconf.h inftrees.h inflate.h inffast.h
 inftrees.o: zutil.h zlib.h zconf.h inftrees.h
 trees.o: deflate.h zutil.h zlib.h zconf.h trees.h
 
-adler32.lo zutil.lo: zutil.h zlib.h zconf.h
+adler32.lo: adler32.c ppc/adler32.c arm/adler32.c x86/adler32.c alpha/adler32.c mips/adler32.c bfin/adler32.c ia64/adler32.c sparc/adler32.c zutil.h zlib.h zconf.h
+zutil.lo: zutil.h zlib.h zconf.h
 gzclose.lo gzlib.lo gzread.lo gzwrite.lo: zlib.h zconf.h gzguts.h
 compress.lo example.lo minigzip.lo uncompr.lo: zlib.h zconf.h
 crc32.lo: zutil.h zlib.h zconf.h crc32.h
diff --git a/adler32.c b/adler32.c
index a868f07..f0e3e47 100644
--- a/adler32.c
+++ b/adler32.c
@@ -1,5 +1,6 @@
 /* adler32.c -- compute the Adler-32 checksum of a data stream
  * Copyright (C) 1995-2011 Mark Adler
+ * Copyright (C) 2010-2011 Jan Seiffert
  * For conditions of distribution and use, see copyright notice in zlib.h
  */
 
@@ -21,6 +22,31 @@ local uLong adler32_combine_ OF((uLong adler1, uLong adler2, z_off64_t len2));
 #define DO8(buf,i)  DO4(buf,i); DO4(buf,i+4);
 #define DO16(buf)   DO8(buf,0); DO8(buf,8);
 
+#if defined(__alpha__)
+/* even if GCC can generate a mul by inverse, the code is really
+ * ugly (find global const pool pointer, load constant, a mul, lots
+ * of shifts/add/sub), up to 14 instructions. The replacement code
+ * only needs >= 5 instructions
+ */
+#  define NO_DIVIDE
+#elif defined(__mips__)
+// TODO: i hate synthetized processors
+/*
+ * If we have a full "high-speed" Multiply/Divide Unit,
+ * the old multiply-by-reciproc should be the best way
+ * (since then we should get a 32x32 mul in 2 cycles?),
+ * but wait, we need 4 muls == 8 + 2 shift + 2 sub + 2 load
+ * imidiate + other.
+ * If we do not have the "full" MDU, a mul takes 32 cycles
+ * and a div 25 (?!?).
+ * GCC generates a classic div, prop. needs the right -mtune
+ * for a mul.
+ * Use our hand rolled reduce, 17 simple instructions for both
+ * operands.
+ */
+#  define NO_DIVIDE
+#endif
+
 /* use NO_DIVIDE if your processor does not do division in hardware --
    try it both ways to see which is faster */
 #ifdef NO_DIVIDE
@@ -56,50 +82,223 @@ local uLong adler32_combine_ OF((uLong adler1, uLong adler2, z_off64_t len2));
         if (a >= BASE) a -= BASE; \
     } while (0)
 #else
+#  define CHOP(a) a %= BASE
 #  define MOD(a) a %= BASE
 #  define MOD28(a) a %= BASE
 #  define MOD63(a) a %= BASE
 #endif
 
+local int host_is_bigendian()
+{
+    local const union {
+        uInt d;
+        unsigned char endian[sizeof(uInt)];
+    } x = {1};
+    return x.endian[0] == 0;
+}
+
+#ifndef NO_ADLER32_VEC
+#  if defined(__arm__)
+#    include "arm/adler32.c"
+#  elif defined(__alpha__)
+#    include "alpha/adler32.c"
+#  elif defined(__bfin__)
+#    include "bfin/adler32.c"
+#  elif defined(__ia64__)
+#    include "ia64/adler32.c"
+#  elif defined(__mips__)
+#    include "mips/adler32.c"
+#  elif defined(__powerpc__) || defined(__powerpc64__)
+#    include "ppc/adler32.c"
+#  elif defined(__sparc) || defined(__sparc__)
+#    include "sparc/adler32.c"
+#  elif defined(__tile__)
+#    include "tile/adler32.c"
+#  elif defined(__i386__) || defined(__x86_64__)
+#    include "x86/adler32.c"
+#  endif
+#endif
+
+#ifndef MIN_WORK
+#  define MIN_WORK 16
+#endif
+
 /* ========================================================================= */
-uLong ZEXPORT adler32(adler, buf, len)
+local noinline uLong adler32_1(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len GCC_ATTR_UNUSED_PARAM;
+{
+    unsigned long sum2;
+
+    /* split Adler-32 into component sums */
+    sum2 = (adler >> 16) & 0xffff;
+    adler &= 0xffff;
+
+    adler += buf[0];
+    if (adler >= BASE)
+        adler -= BASE;
+    sum2 += adler;
+    if (sum2 >= BASE)
+        sum2 -= BASE;
+    return adler | (sum2 << 16);
+}
+
+/* ========================================================================= */
+local noinline uLong adler32_common(adler, buf, len)
     uLong adler;
     const Bytef *buf;
     uInt len;
 {
     unsigned long sum2;
-    unsigned n;
 
     /* split Adler-32 into component sums */
     sum2 = (adler >> 16) & 0xffff;
     adler &= 0xffff;
 
-    /* in case user likes doing a byte at a time, keep it fast */
-    if (len == 1) {
-        adler += buf[0];
-        if (adler >= BASE)
-            adler -= BASE;
+    while (len--) {
+        adler += *buf++;
         sum2 += adler;
-        if (sum2 >= BASE)
-            sum2 -= BASE;
-        return adler | (sum2 << 16);
     }
+    if (adler >= BASE)
+        adler -= BASE;
+    MOD28(sum2);             /* only added so many BASE's */
+    return adler | (sum2 << 16);
+}
 
-    /* initial Adler-32 value (deferred check for len == 1 speed) */
-    if (buf == Z_NULL)
-        return 1L;
+#ifndef HAVE_ADLER32_VEC
+#  if (defined(__LP64__) || ((SIZE_MAX-0) >> 31) >= 2) && !defined(NO_ADLER32_VEC)
 
-    /* in case short lengths are provided, keep it somewhat fast */
-    if (len < 16) {
-        while (len--) {
-            adler += *buf++;
-            sum2 += adler;
-        }
-        if (adler >= BASE)
-            adler -= BASE;
-        MOD28(sum2);            /* only added so many BASE's */
-        return adler | (sum2 << 16);
-    }
+/* On 64 Bit archs, we can do pseudo SIMD with a nice win.
+ * This is esp. important for old Alphas, they do not have byte
+ * access.
+ * This needs some register but x86_64 is fine (>= 9 for the mainloop
+ * req.). If your 64 Bit arch is more limited, throw it away...
+ */
+#    undef VNMAX
+#    define VNMAX (2*NMAX+((9*NMAX)/10))
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1, s2;
+    unsigned int k;
+
+    /* split Adler-32 into component sums */
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    /* align input data */
+    k    = ALIGN_DIFF(buf, sizeof(size_t));
+    len -= k;
+    if (k) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while(--k);
+
+    k = len > VNMAX ? VNMAX : len;
+    len -= k;
+    if (likely(k >= 2 * sizeof(size_t))) do
+    {
+        unsigned int vs1, vs2;
+        unsigned int vs1s;
+
+        /* add s1 to s2 for rounds to come */
+        s2 += s1 * ROUND_TO(k, sizeof(size_t));
+        vs1s = vs1 = vs2 = 0;
+        do {
+            size_t vs1l = 0, vs1h = 0, vs1l_s = 0, vs1h_s = 0;
+            unsigned int a, b, c, d, e, f, g, h;
+            unsigned int j;
+
+            j = k > 23 * sizeof(size_t) ? 23 : k/sizeof(size_t);
+            k -= j * sizeof(size_t);
+            /* add s1 to s1 round sum for rounds to come */
+            vs1s += j * vs1;
+            do {
+                size_t in8 = *(const size_t *)buf;
+                buf += sizeof(size_t);
+                /* add this s1 to s1 round sum */
+                vs1l_s += vs1l;
+                vs1h_s += vs1h;
+                /* add up input data to s1 */
+                vs1l +=  in8 & UINT64_C(0x00ff00ff00ff00ff);
+                vs1h += (in8 & UINT64_C(0xff00ff00ff00ff00)) >> 8;
+            } while(--j);
+
+            /* split s1 */
+            if(host_is_bigendian()) {
+                a = (vs1h >> 48) & 0x0000ffff;
+                b = (vs1l >> 48) & 0x0000ffff;
+                c = (vs1h >> 32) & 0x0000ffff;
+                d = (vs1l >> 32) & 0x0000ffff;
+                e = (vs1h >> 16) & 0x0000ffff;
+                f = (vs1l >> 16) & 0x0000ffff;
+                g = (vs1h      ) & 0x0000ffff;
+                h = (vs1l      ) & 0x0000ffff;
+            } else {
+                a = (vs1l      ) & 0x0000ffff;
+                b = (vs1h      ) & 0x0000ffff;
+                c = (vs1l >> 16) & 0x0000ffff;
+                d = (vs1h >> 16) & 0x0000ffff;
+                e = (vs1l >> 32) & 0x0000ffff;
+                f = (vs1h >> 32) & 0x0000ffff;
+                g = (vs1l >> 48) & 0x0000ffff;
+                h = (vs1h >> 48) & 0x0000ffff;
+            }
+
+            /* add s1 & s2 horiz. */
+            vs2 += 8*a + 7*b + 6*c + 5*d + 4*e + 3*f + 2*g + 1*h;
+            vs1 += a + b + c + d + e + f + g + h;
+
+            /* split and add up s1 round sum */
+            vs1l_s = ((vs1l_s      ) & UINT64_C(0x0000ffff0000ffff)) +
+                     ((vs1l_s >> 16) & UINT64_C(0x0000ffff0000ffff));
+            vs1h_s = ((vs1h_s      ) & UINT64_C(0x0000ffff0000ffff)) +
+                     ((vs1h_s >> 16) & UINT64_C(0x0000ffff0000ffff));
+            vs1l_s += vs1h_s;
+            vs1s += ((vs1l_s      ) & UINT64_C(0x00000000ffffffff)) +
+                    ((vs1l_s >> 32) & UINT64_C(0x00000000ffffffff));
+        } while (k >= sizeof(size_t));
+        CHOP(vs1s);
+        s2 += vs1s * 8 + vs2;
+        CHOP(s2);
+        s1 += vs1;
+        CHOP(s1);
+        len += k;
+        k = len > VNMAX ? VNMAX : len;
+        len -= k;
+    } while (k >= sizeof(size_t));
+
+    /* handle trailer */
+    if (k) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--k);
+    MOD28(s1);
+    MOD28(s2);
+
+    /* return recombined sums */
+    return (s2 << 16) | s1;
+}
+
+#  else
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned long sum2;
+    unsigned n;
+
+    /* split Adler-32 into component sums */
+    sum2 = (adler >> 16) & 0xffff;
+    adler &= 0xffff;
 
     /* do length NMAX blocks -- requires just one modulo operation */
     while (len >= NMAX) {
@@ -131,6 +330,73 @@ uLong ZEXPORT adler32(adler, buf, len)
     /* return recombined sums */
     return adler | (sum2 << 16);
 }
+#  endif
+#endif
+
+/* ========================================================================= */
+#if MIN_WORK - 16 > 0
+#  ifndef NO_ADLER32_GE16
+local noinline uLong adler32_ge16(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned long sum2;
+    unsigned n;
+
+    /* split Adler-32 into component sums */
+    sum2 = (adler >> 16) & 0xffff;
+    adler &= 0xffff;
+    n = len / 16;
+    len %= 16;
+
+    do {
+        DO16(buf); /* 16 sums unrolled */
+        buf += 16;
+    } while (--n);
+
+    /* handle trailer */
+    while (len--) {
+        adler += *buf++;
+        sum2 += adler;
+    }
+
+    MOD28(adler);
+    MOD28(sum2);
+
+    /* return recombined sums */
+    return adler | (sum2 << 16);
+}
+#  endif
+#  define COMMON_WORK 16
+#else
+#  define COMMON_WORK MIN_WORK
+#endif
+
+/* ========================================================================= */
+uLong ZEXPORT adler32(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    /* in case user likes doing a byte at a time, keep it fast */
+    if (len == 1)
+        return adler32_1(adler, buf, len); /* should create a fast tailcall */
+
+    /* initial Adler-32 value (deferred check for len == 1 speed) */
+    if (buf == Z_NULL)
+        return 1L;
+
+    /* in case short lengths are provided, keep it somewhat fast */
+    if (len < COMMON_WORK)
+        return adler32_common(adler, buf, len);
+#if MIN_WORK - 16 > 0
+    if (len < MIN_WORK)
+        return adler32_ge16(adler, buf, len);
+#endif
+
+    return adler32_vec(adler, buf, len);
+}
 
 /* ========================================================================= */
 local uLong adler32_combine_(adler1, adler2, len2)
diff --git a/alpha/adler32.c b/alpha/adler32.c
new file mode 100644
index 0000000..c20b232
--- /dev/null
+++ b/alpha/adler32.c
@@ -0,0 +1,136 @@
+/*
+ * adler32.c -- compute the Adler-32 checksum of a data stream
+ *   alpha implementation
+ * Copyright (C) 1995-2007 Mark Adler
+ * Copyright (C) 2010-2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* @(#) $Id$ */
+
+#if defined(__GNUC__) && defined(__alpha_max__)
+#  define HAVE_ADLER32_VEC
+#  define MIN_WORK 32
+#  define VNMAX (2*NMAX+((9*NMAX)/10))
+
+#  define SOUL (sizeof(unsigned long))
+
+#  if GCC_VERSION_GE(303)
+#    define unpkbw  __builtin_alpha_unpkbw
+#    define perr    __builtin_alpha_perr
+#  else
+/* ========================================================================= */
+local inline unsigned long unpkbw(unsigned long a)
+{
+    unsigned long r;
+    __asm__ (".arch ev6; unpkbw	%r1, %0" : "=r" (r) : "rJ" (a));
+    return r;
+}
+
+/* ========================================================================= */
+local inline unsigned long perr(unsigned long a, unsigned long b)
+{
+    unsigned long r;
+    __asm__ (".arch ev6; perr	%r1, %r2, %0" : "=r" (r) : "rJ" (a), "rJ" (b));
+    return r;
+}
+#  endif
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1, s2;
+    unsigned k;
+
+    /* split Adler-32 into component sums */
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    /* align input */
+    k    = ALIGN_DIFF(buf, SOUL);
+    len -= k;
+    if (k) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--k);
+
+    k = len < VNMAX ? len : VNMAX;
+    len -= k;
+    if (likely(k >= 2 * SOUL)) {
+        unsigned long vs1 = s1, vs2 = s2;
+
+        do {
+            unsigned long vs1_r = 0;
+            do {
+                unsigned long a, b, c, d, e, f, g, h;
+                unsigned long vs2l = 0, vs2h = 0;
+                unsigned j;
+
+                j = k > 257 * SOUL ? 257 : k/SOUL;
+                k -= j * SOUL;
+                do {
+                    /* get input data */
+                    unsigned long in = *(const unsigned long *)buf;
+                    /* add vs1 for this round */
+                    vs1_r += vs1;
+                    /* add horizontal */
+                    vs1 += perr(in, 0);
+                    /* extract */
+                    vs2l += unpkbw(in);
+                    vs2h += unpkbw(in >> 32);
+                    buf += SOUL;
+                } while (--j);
+                /* split vs2 */
+                if(host_is_bigendian()) {
+                    a = (vs2h >> 48) & 0x0000ffff;
+                    b = (vs2h >> 32) & 0x0000ffff;
+                    c = (vs2h >> 16) & 0x0000ffff;
+                    d = (vs2h      ) & 0x0000ffff;
+                    e = (vs2l >> 48) & 0x0000ffff;
+                    f = (vs2l >> 32) & 0x0000ffff;
+                    g = (vs2l >> 16) & 0x0000ffff;
+                    h = (vs2l      ) & 0x0000ffff;
+                } else {
+                    a = (vs2l      ) & 0x0000ffff;
+                    b = (vs2l >> 16) & 0x0000ffff;
+                    c = (vs2l >> 32) & 0x0000ffff;
+                    d = (vs2l >> 48) & 0x0000ffff;
+                    e = (vs2h      ) & 0x0000ffff;
+                    f = (vs2h >> 16) & 0x0000ffff;
+                    g = (vs2h >> 32) & 0x0000ffff;
+                    h = (vs2h >> 48) & 0x0000ffff;
+                }
+                /* mull&add vs2 horiz. */
+                vs2 += 8*a + 7*b + 6*c + 5*d + 4*e + 3*f + 2*g + 1*h;
+            } while (k >= SOUL);
+            /* chop vs1 round sum before multiplying by 8 */
+            CHOP(vs1_r);
+            /* add vs1 for this round (8 times) */
+            vs2 += vs1_r * 8;
+            /* CHOP both sums */
+            CHOP(vs2);
+            CHOP(vs1);
+            len += k;
+            k = len < VNMAX ? len : VNMAX;
+            len -= k;
+        } while (likely(k >= SOUL));
+        s1 = vs1;
+        s2 = vs2;
+    }
+
+    /* handle trailer */
+    if (unlikely(k)) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--k);
+    /* at this point we should not have so big s1 & s2 */
+    MOD28(s1);
+    MOD28(s2);
+
+    /* return recombined sums */
+    return (s2 << 16) | s1;
+}
+#endif
diff --git a/arm/adler32.c b/arm/adler32.c
new file mode 100644
index 0000000..d23bfb0
--- /dev/null
+++ b/arm/adler32.c
@@ -0,0 +1,614 @@
+/*
+ * adler32.c -- compute the Adler-32 checksum of a data stream
+ *   arm implementation
+ * Copyright (C) 1995-2007 Mark Adler
+ * Copyright (C) 2009-2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* @(#) $Id$ */
+
+#if defined(__ARM_NEON__) && defined(__ARMEL__)
+/*
+ * Big endian NEON qwords are kind of broken.
+ * They are big endian within the dwords, but WRONG
+ * (really??) way round between lo and hi.
+ * Creating some kind of PDP11 middle endian.
+ *
+ * This is madness and unsupportable. For this reason
+ * GCC wants to disable qword endian specific patterns.
+ */
+#  include <arm_neon.h>
+
+#  define SOVUCQ sizeof(uint8x16_t)
+#  define SOVUC sizeof(uint8x8_t)
+/* since we do not have the 64bit psadbw sum, we could still go a little higher (we are at 0xc) */
+#  define VNMAX (8*NMAX)
+#  define HAVE_ADLER32_VEC
+#  define MIN_WORK 32
+
+/* ========================================================================= */
+local inline uint8x16_t neon_simple_alignq(uint8x16_t a, uint8x16_t b, unsigned amount)
+{
+    switch(amount % SOVUCQ)
+    {
+    case  0: return a;
+    case  1: return vextq_u8(a, b,  1);
+    case  2: return vextq_u8(a, b,  2);
+    case  3: return vextq_u8(a, b,  3);
+    case  4: return vextq_u8(a, b,  4);
+    case  5: return vextq_u8(a, b,  5);
+    case  6: return vextq_u8(a, b,  6);
+    case  7: return vextq_u8(a, b,  7);
+    case  8: return vextq_u8(a, b,  8);
+    case  9: return vextq_u8(a, b,  9);
+    case 10: return vextq_u8(a, b, 10);
+    case 11: return vextq_u8(a, b, 11);
+    case 12: return vextq_u8(a, b, 12);
+    case 13: return vextq_u8(a, b, 13);
+    case 14: return vextq_u8(a, b, 14);
+    case 15: return vextq_u8(a, b, 15);
+    }
+    return b;
+}
+
+/* ========================================================================= */
+local inline uint32x4_t vector_chop(uint32x4_t x)
+{
+    uint32x4_t y;
+
+    y = vshlq_n_u32(x, 16);
+    x = vshrq_n_u32(x, 16);
+    y = vshrq_n_u32(y, 16);
+    y = vsubq_u32(y, x);
+    x = vaddq_u32(y, vshlq_n_u32(x, 4));
+    return x;
+}
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    uint32x4_t v0_32 = (uint32x4_t){0,0,0,0};
+    uint8x16_t    v0 = (uint8x16_t)v0_32;
+    uint8x16_t vord, vord_a;
+    uint32x4_t vs1, vs2;
+    uint32x2_t v_tsum;
+    uint8x16_t in16;
+    uint32_t s1, s2;
+    unsigned k;
+
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    vord = (uint8x16_t){16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1};
+
+    if (likely(len >= 2*SOVUCQ)) {
+        unsigned f, n;
+
+        /*
+         * Add stuff to achieve alignment
+         */
+        /* align hard down */
+        f = (unsigned) ALIGN_DOWN_DIFF(buf, SOVUCQ);
+        n = SOVUCQ - f;
+        buf = (const unsigned char *)ALIGN_DOWN(buf, SOVUCQ);
+
+        /* add n times s1 to s2 for start round */
+        s2 += s1 * n;
+
+        /* set sums 0 */
+        vs1 = v0_32;
+        vs2 = v0_32;
+        /*
+         * the accumulation of s1 for every round grows very fast
+         * (quadratic?), even if we accumulate in 4 dwords, more
+         * rounds means nonlinear growth.
+         * We already split it out of s2, normaly it would be in
+         * s2 times 16... and even grow faster.
+         * Thanks to this split and vector reduction, we can stay
+         * longer in the loops. But we have to prepare for the worst
+         * (all 0xff), only do 6 times the work.
+         * (we could prop. stay a little longer since we have 4 sums,
+         * not 2 like on x86).
+         */
+        k = len < VNMAX ? (unsigned)len : VNMAX;
+        len -= k;
+        /* insert scalar start somewhere */
+        vs1 = vsetq_lane_u32(s1, vs1, 0);
+        vs2 = vsetq_lane_u32(s2, vs2, 0);
+
+        /* get input data */
+        in16 = *(const uint8x16_t *)buf;
+        /* mask out excess data */
+        in16 = neon_simple_alignq(in16, v0, f);
+        vord_a = neon_simple_alignq(vord, v0, f);
+        /* pairwise add bytes and long, pairwise add word long acc */
+        vs1 = vpadalq_u16(vs1, vpaddlq_u8(in16));
+        /* apply order, add words, pairwise add word long acc */
+        vs2 = vpadalq_u16(vs2,
+                vmlal_u8(
+                    vmull_u8(vget_low_u8(in16), vget_low_u8(vord_a)),
+                    vget_high_u8(in16), vget_high_u8(vord_a)
+                    )
+                );
+
+        buf += SOVUCQ;
+        k -= n;
+
+        if (likely(k >= SOVUCQ)) do {
+            uint32x4_t vs1_r = v0_32;
+            do {
+                uint16x8_t vs2_lo = (uint16x8_t)v0_32, vs2_hi = (uint16x8_t)v0_32;
+                unsigned j;
+
+                j  = (k/16) > 16 ? 16 : k/16;
+                k -= j * 16;
+                do {
+                    /* GCC does not create the most pretty inner loop,
+                     * with extra moves and stupid scheduling, but
+                     * i am not in the mood for inline ASM, keep it
+                     * compatible.
+                     */
+                    /* get input data */
+                    in16 = *(const uint8x16_t *)buf;
+                    buf += SOVUCQ;
+
+                    /* add vs1 for this round */
+                    vs1_r = vaddq_u32(vs1_r, vs1);
+
+                    /* pairwise add bytes and long, pairwise add word long acc */
+                    vs1 = vpadalq_u16(vs1, vpaddlq_u8(in16));
+                    /* apply order, word long and acc */
+                    vs2_lo = vmlal_u8(vs2_lo, vget_low_u8(in16), vget_low_u8(vord));
+                    vs2_hi = vmlal_u8(vs2_hi, vget_high_u8(in16), vget_high_u8(vord));
+                } while(--j);
+                /* pair wise add long and acc */
+                vs2 = vpadalq_u16(vs2, vs2_lo);
+                vs2 = vpadalq_u16(vs2, vs2_hi);
+            } while (k >= SOVUCQ);
+            /* chop vs1 round sum before multiplying by 16 */
+            vs1_r = vector_chop(vs1_r);
+            /* add vs1 for this round (16 times) */
+            /* they have shift right and accummulate, where is shift left and acc?? */
+            vs2 = vaddq_u32(vs2, vshlq_n_u32(vs1_r, 4));
+            /* chop both vectors to something within 16 bit */
+            vs2 = vector_chop(vs2);
+            vs1 = vector_chop(vs1);
+            len += k;
+            k = len < VNMAX ? (unsigned) len : VNMAX;
+            len -= k;
+        } while (likely(k >= SOVUCQ));
+
+        if (likely(k)) {
+            /*
+             * handle trailer
+             */
+            f = SOVUCQ - k;
+            /* add k times vs1 for this trailer */
+            vs2 = vmlaq_u32(vs2, vs1, vdupq_n_u32(k));
+
+            /* get input data */
+            in16 = *(const uint8x16_t *)buf;
+            /* masks out bad data */
+            in16 = neon_simple_alignq(v0, in16, k);
+
+            /* pairwise add bytes and long, pairwise add word long acc */
+            vs1 = vpadalq_u16(vs1, vpaddlq_u8(in16));
+            /* apply order, add words, pairwise add word long acc */
+            vs2 = vpadalq_u16(vs2,
+                    vmlal_u8(
+                        vmull_u8(vget_low_u8(in16), vget_low_u8(vord)),
+                        vget_high_u8(in16), vget_high_u8(vord)
+                        )
+                    );
+
+            buf += k;
+            k -= k;
+        }
+
+        /* add horizontal */
+        v_tsum = vpadd_u32(vget_high_u32(vs1), vget_low_u32(vs1));
+        v_tsum = vpadd_u32(v_tsum, v_tsum);
+        s1 = vget_lane_u32(v_tsum, 0);
+        v_tsum = vpadd_u32(vget_high_u32(vs2), vget_low_u32(vs2));
+        v_tsum = vpadd_u32(v_tsum, v_tsum);
+        s2 = vget_lane_u32(v_tsum, 0);
+    }
+
+    if (unlikely(len)) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--len);
+    MOD28(s1);
+    MOD28(s2);
+
+    return (s2 << 16) | s1;
+}
+
+#elif defined(__IWMMXT__)
+#  ifndef __GNUC__
+/* GCC doesn't take it's own intrinsic header and ICEs if forced to */
+#    include <mmintrin.h>
+#  else
+typedef unsigned long long __m64;
+
+// TODO: older gcc may need U constrain instead of y?
+static inline __m64 _mm_setzero_si64(void)
+{
+    __m64 r;
+#  if 0
+    asm ("wzero %0" : "=y" (r));
+#  else
+    r = 0;
+#  endif
+    return r;
+}
+/* there is slli/srli and we want to use it, but it's iWMMXt-2 */
+static inline __m64 _mm_sll_pi32(__m64 a, __m64 c)
+{
+    asm ("wsllw %0, %1, %2" : "=y" (a) : "y" (a), "y" (c));
+    return a;
+}
+static inline __m64 _mm_srl_pi32(__m64 a, __m64 c)
+{
+    asm ("wsrlw %0, %1, %2" : "=y" (a) : "y" (a), "y" (c));
+    return a;
+}
+static inline __m64 _mm_sub_pi32(__m64 a, __m64 b)
+{
+    asm ("wsubw %0, %1, %2" : "=y" (a) : "y" (a), "y" (b));
+    return a;
+}
+static inline __m64 _mm_add_pi16(__m64 a, __m64 b)
+{
+    asm ("waddh %0, %1, %2" : "=y" (a) : "y" (a), "y" (b));
+    return a;
+}
+static inline __m64 _mm_add_pi32(__m64 a, __m64 b)
+{
+    asm ("waddw %0, %1, %2" : "=y" (a) : "y" (a), "y" (b));
+    return a;
+}
+static inline __m64 _mm_sada_pu8(__m64 acc, __m64 a, __m64 b)
+{
+    asm ("wsadb %0, %1, %2" : "=y" (acc) : "y" (a), "y" (b), "0" (acc));
+    return acc;
+}
+static inline __m64 _mm_madd_pu16(__m64 a, __m64 b)
+{
+    asm ("wmaddu %0, %1, %2" : "=y" (a) : "y" (a), "y" (b));
+    return a;
+}
+static inline __m64 _mm_mac_pu16(__m64 acc, __m64 a, __m64 b)
+{
+    asm ("wmacu %0, %1, %2" : "=y" (acc) : "y" (a), "y" (b), "0" (acc));
+    return acc;
+}
+static inline __m64 _mm_unpackel_pu8(__m64 a)
+{
+    asm ("wunpckelub %0, %1" : "=y" (a) : "y" (a));
+    return a;
+}
+static inline __m64 _mm_unpackeh_pu8(__m64 a)
+{
+    asm ("wunpckehub %0, %1" : "=y" (a) : "y" (a));
+    return a;
+}
+static inline __m64 _mm_shuffle_pi16(__m64 a, const int m)
+{
+    asm ("wshufh %0, %1, %2" : "=y" (a) : "y" (a), "i" (m));
+    return a;
+}
+static inline unsigned int _mm_extract_pu32(__m64 a, const int m)
+{
+    unsigned int r;
+    asm ("textrmuw %0, %1, %2" : "=r" (r) : "y" (a), "i" (m));
+    return r;
+}
+static inline __m64 _mm_insert_pi32(__m64 a, unsigned int b, const int m)
+{
+    asm ("tinsrw %0, %1, %2" : "=y" (a) : "r" (b), "i" (m), "0" (a));
+    return a;
+}
+static inline __m64 _mm_align_si64(__m64 a, __m64 b, int c)
+{
+    asm ("walignr%U3 %0, %1, %2" : "=y" (a) : "y" (a), "y" (b), "z" (c));
+    return a;
+}
+static inline __m64 _mm_set_pi16(short a, short b, short c, short d)
+{
+    __m64 r = (unsigned long long)d;
+    r |= ((unsigned long long)c) << 16;
+    r |= ((unsigned long long)b) << 32;
+    r |= ((unsigned long long)a) << 48;
+    return r;
+}
+#  endif
+
+// TODO: we could go over NMAX, since we have split the vs2 sum
+/* but we shuffle vs1_r only every 2056 byte, so we can not go full */
+#  define VNMAX (3*NMAX)
+#  define HAVE_ADLER32_VEC
+#  define MIN_WORK 32
+#  define SOV8 (sizeof(__m64))
+
+/* ========================================================================= */
+local inline __m64 vector_chop(__m64 x)
+{
+    static const __m64 four = 4;
+    static const __m64 sixten = 16;
+    __m64 y = _mm_sll_pi32(x, sixten);
+    x = _mm_srl_pi32(x, sixten);
+    y = _mm_srl_pi32(y, sixten);
+    y = _mm_sub_pi32(y, x);
+    x = _mm_add_pi32(y, _mm_sll_pi32(x, four));
+    return x;
+}
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1, s2;
+    unsigned int k;
+
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    if (likely(len >= 4 * SOV8)) {
+        static const __m64 three = 3;
+        __m64 vs1, vs2;
+        __m64 vzero;
+        __m64 vorder_l, vorder_h;
+        unsigned int f, n;
+
+        vzero = _mm_setzero_si64();
+
+        /* align hard down */
+        f = (unsigned int) ALIGN_DOWN_DIFF(buf, SOV8);
+        buf = (const Bytef *)ALIGN_DOWN(buf, SOV8);
+        n = SOV8 - f;
+
+        /* add n times s1 to s2 for start round */
+        s2 += s1 * n;
+
+        k = len < VNMAX ? len : VNMAX;
+        len -= k;
+
+        /* insert scalar start */
+        vs1 = _mm_insert_pi32(vzero, s1, 0);
+        vs2 = _mm_insert_pi32(vzero, s2, 0);
+
+// TODO: byte order?
+        if (host_is_bigendian()) {
+            vorder_l = _mm_set_pi16(4, 3, 2, 1);
+            vorder_h = _mm_set_pi16(8, 7, 6, 5);
+        } else {
+            vorder_l = _mm_set_pi16(5, 6, 7, 8);
+            vorder_h = _mm_set_pi16(1, 2, 3, 4);
+        }
+
+        {
+            __m64 in = *(const __m64 *)buf;
+
+            /* mask excess info out */
+            if (host_is_bigendian()) {
+                in = _mm_align_si64(vzero, in, n);
+                in = _mm_align_si64(in, vzero, f);
+            } else {
+                in = _mm_align_si64(in, vzero, f);
+                in = _mm_align_si64(vzero, in, n);
+            }
+
+            /* add horizontal and acc */
+            vs1 = _mm_sada_pu8(vs1, in, vzero);
+
+            /* widen bytes to words, apply order and acc */
+            vs2 = _mm_mac_pu16(vs2, _mm_unpackel_pu8(in), vorder_l);
+            vs2 = _mm_mac_pu16(vs2, _mm_unpackeh_pu8(in), vorder_h);
+        }
+
+        buf += SOV8;
+        k -= n;
+
+        do {
+            __m64 vs1_r = vzero;
+
+            do {
+                __m64 vs2_l = vzero, vs2_h = vzero;
+                unsigned int j;
+
+                j  = k >= (257 * SOV8) ? 257 * SOV8 : k;
+                j /= SOV8;
+                k -= j * SOV8;
+                do {
+                    /* get input data */
+                    __m64 in = *(const __m64 *)buf;
+                    buf += SOV8;
+
+                    /* add vs1 for this round */
+                    vs1_r = _mm_add_pi32(vs1_r, vs1);
+
+                    /* add horizontal and acc */
+// TODO: how does wsad really work?
+                    /*
+                     * the Intel iwmmxt 1 & 2 manual says the wsad instruction
+                     * always zeros the upper word (32 in the arm context),
+                     * and then adds all sad into the lower word (again 32
+                     * bit). If the z version is choosen, the lower word is
+                     * also zeroed before, otherwise we get an acc.
+                     *
+                     * Visual studio only knows the sada intrinsic to reflect
+                     * that, but no description, no prototype.
+                     *
+                     * But there is no sada intrinsic in the Intel manual.
+                     * The Intel iwmmxt-1 manual only knows sad & sadz, two
+                     * operands, instead the acc is done with the lvalue
+                     * (which only really works with spec. compiler builtins).
+                     * GCC follows the intel manual (but does gcc manages to
+                     * use the lvalue?).
+                     * To make matters worse the description for the _mm_sad_pu8
+                     * intrinsic says it clears the upper _3_ fields, and only
+                     * acc in the lowest, so only working in 16 Bit.
+                     * So who is wrong?
+                     *
+                     * If this is different between 1 & 2 we are screwed, esp.
+                     * since i can not find a preprocessor define if 1 or 2.
+                     */
+                    vs1 = _mm_sada_pu8(vs1, in, vzero);
+
+                    /* widen bytes to words and acc */
+                    vs2_l = _mm_add_pi16(vs2_l, _mm_unpackel_pu8(in));
+                    vs2_h = _mm_add_pi16(vs2_h, _mm_unpackeh_pu8(in));
+                } while (--j);
+                /* shake and roll vs1_r, so both 32 bit sums get some input */
+                vs1_r = _mm_shuffle_pi16(vs1_r, 0x4e);
+                /* apply order and add to 32 bit */
+                vs2_l = _mm_madd_pu16(vs2_l, vorder_l);
+                vs2_h = _mm_madd_pu16(vs2_h, vorder_h);
+                /* acc */
+                vs2 = _mm_add_pi32(vs2, vs2_l);
+                vs2 = _mm_add_pi32(vs2, vs2_h);
+            } while (k >= SOV8);
+            /* chop vs1 round sum before multiplying by 8 */
+            vs1_r = vector_chop(vs1_r);
+            /* add vs1 for this round (8 times) */
+            vs2 = _mm_add_pi32(vs2, _mm_sll_pi32(vs1_r, three));
+            /* chop both sums to something within 16 bit */
+            vs2 = vector_chop(vs2);
+            vs1 = vector_chop(vs1);
+            len += k;
+            k = len < VNMAX ? len : VNMAX;
+            len -= k;
+        } while (likely(k >= SOV8));
+        len += k;
+        vs1 = _mm_add_pi32(vs1, _mm_shuffle_pi16(vs1, 0x4e));
+        vs2 = _mm_add_pi32(vs2, _mm_shuffle_pi16(vs2, 0x4e));
+        s1 = _mm_extract_pu32(vs1, 0);
+        s2 = _mm_extract_pu32(vs2, 0);
+    }
+
+    if (unlikely(len)) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--len);
+    /* at this point we should have not so big s1 & s2 */
+    MOD28(s1);
+    MOD28(s2);
+
+    return (s2 << 16) | s1;
+}
+
+/* inline asm, so only on GCC (or compatible) && ARM v6 or better */
+#elif 0 && defined(__GNUC__) && ( \
+        defined(__thumb2__)  && ( \
+            !defined(__ARM_ARCH_7__) && !defined(__ARM_ARCH_7M__) \
+        ) || ( \
+        !defined(__thumb__) && ( \
+            defined(__ARM_ARCH_6__)   || defined(__ARM_ARCH_6J__)  || \
+            defined(__ARM_ARCH_6T2__) || defined(__ARM_ARCH_6ZK__) || \
+            defined(__ARM_ARCH_7A__)  || defined(__ARM_ARCH_7R__) \
+        )) \
+    )
+/* This code is disabled, since it is not faster, only for reference.
+ * We are at speedup: 0.952830
+ * Again counting instructions is futile, 5 instructions per 4 bytes
+ * against at least 3 per byte (loop overhead excluded) is no win.
+ * And split sums also does not save us.
+ */
+#  define SOU32 (sizeof(unsigned int))
+#  define HAVE_ADLER32_VEC
+#  define MIN_WORK 16
+// TODO: maybe 2*NMAX is possible, but that's very thin
+/* this way we are at 0xda */
+#  define VNMAX (NMAX+((NMAX*9)/10))
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1, s2;
+    unsigned int k;
+
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    k    = ALIGN_DIFF(buf, SOU32);
+    len -= k;
+    if (k) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--k);
+
+    if (likely(len >= 4 * SOU32)) {
+        unsigned int vs1 = s1, vs2 = s2;
+        unsigned int order_lo, order_hi;
+
+        if (host_is_bigendian()) {
+            order_lo = 0x00030001;
+            order_hi = 0x00040002;
+        } else {
+            order_lo = 0x00020004;
+            order_hi = 0x00010003;
+        }
+        k = len < VNMAX ? len : VNMAX;
+        len -= k;
+
+        do {
+            unsigned int vs1_r = 0;
+            do {
+                unsigned int j;
+                unsigned int vs2_lo = 0, vs2_hi = 0;
+
+                j  = (k/4) >= 128 ? 128 : (k/4);
+                k -= j * 4;
+                do {
+                    /* get input data */
+                    unsigned int in = *(const unsigned int *)buf;
+                    buf += SOU32;
+                    /* add vs1 for this round */
+                    vs1_r += vs1;
+                    /* add horizontal and acc */
+                    asm ("usada8 %0, %1, %2, %3" : "=r" (vs1) : "r" (in), "r" (0), "r" (vs1));
+                    /* widen bytes to words and acc */
+                    asm ("uxtab16 %0, %1, %2" : "=r" (vs2_lo) : "r" (vs2_lo), "r" (in));
+                    asm ("uxtab16 %0, %1, %2, ror #8" : "=r" (vs2_hi) : "r" (vs2_hi), "r" (in));
+                } while (--j);
+                /* aply order and acc */
+                asm ("smlad %0, %1, %2, %3" : "=r" (vs2) : "r" (vs2_lo) , "r" (order_lo), "r" (vs2));
+                asm ("smlad %0, %1, %2, %3" : "=r" (vs2) : "r" (vs2_hi) , "r" (order_hi), "r" (vs2));
+            } while (k >= SOU32);
+            /* chop vs1 round sum before multiplying by 4 */
+            CHOP(vs1_r);
+            /* add vs1 for this round (4 times) */
+            vs2 += vs1_r * 4;
+            /* chop both sums */
+            CHOP(vs2);
+            CHOP(vs1);
+            len += k;
+            k = len < VNMAX ? len : VNMAX;
+            len -= k;
+        } while (likely(k >= SOU32));
+        len += k;
+        s1 = vs1;
+        s2 = vs2;
+    }
+
+    if (unlikely(len)) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--len);
+    /* at this point we should not have so big s1 & s2 */
+    MOD28(s1);
+    MOD28(s2);
+
+    return (s2 << 16) | s1;
+}
+#endif
diff --git a/bfin/adler32.c b/bfin/adler32.c
new file mode 100644
index 0000000..36a2f9d
--- /dev/null
+++ b/bfin/adler32.c
@@ -0,0 +1,195 @@
+/*
+ * adler32.c -- compute the Adler-32 checksum of a data stream
+ *   blackfin implementation
+ * Copyright (C) 1995-2007 Mark Adler
+ * Copyright (C) 2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* @(#) $Id$ */
+
+/* we use inline asm, so GCC it is */
+#ifdef __GNUC__
+#  define HAVE_ADLER32_VEC
+#  define MIN_WORK 16
+#  define VNMAX (2*NMAX + ((9*NMAX)/10))
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    struct vord { unsigned long d[4]; };
+    static const struct vord vord_e = {{0x00080001, 0x00060001, 0x00040001, 0x00020001}};
+    static const struct vord vord_o = {{0x00070001, 0x00050001, 0x00030001, 0x00010001}};
+    unsigned long s1, s2, s1s;
+    unsigned long lt0, lt1;
+    unsigned long fo, fe, fex;
+    unsigned long lcount, pt;
+
+    /* split Adler-32 into component sums */
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    __asm__ __volatile__ (
+        /* setup circular adressing */
+        "I0 = %[lt1];\n\t"
+        "B0 = %[lt1];\n\t"
+        "%[pt] = %[len];\n\t"
+        "I2 = %[vord_e];\n\t"
+        "B2 = I2;\n\t"
+        "%[pt] += 4\n\t"
+        "L2 = %[sorder];\n\t"
+        "I3 = %[vord_o];\n\t"
+        "L0 = %[pt];\n\t"
+        "B3 = I3;\n\t"
+        "L3 = L2;\n\t"
+        /* fetch start data */
+        "DISALGNEXCPT || %[lt0] = [I0 ++ %[m_4]];\n\t"
+        "%[fe] = [I2 ++ %[m_4]];\n\t"
+        "%[fo] = [I3 ++ %[m_4]];\n"
+        /****************************/
+        "1:\n\t"
+        /* put sums into ACC */
+        "A1 = %0;\n\t"
+        "A0 = %1;\n\t"
+        /* prepare len and hw loop counter */
+        "%[lcount] = %[vnmax] (Z);\n\t"
+        "%[pt] = %[len];\n\t"
+        "%[pt] += -1;\n\t"
+        "CC = %[lcount] <= %[pt] (IU);\n\t"
+        "IF !CC %[lcount] = %[pt];\n"
+        "%[lcount] = %[lcount] >> 2;\n\t"
+        "%[lcount] = %[lcount] >> 1;\n\t"
+        "%[pt] = %[lcount] << 2;\n\t"
+        "%[pt] = %[pt] << 1;\n\t"
+        "%[len] -= %[pt];\n\t"
+        "%[pt] = 8;\n\t"
+        "%2 = 0;\n\t" /* s1s = 0 */
+        "LOOP inner_add LC1 = %[lcount];\n\t"
+        /*=========================*/
+        "LOOP_BEGIN inner_add;\n\t"
+        "%2 = %2 + %0;\n\t"
+        "DISALGNEXCPT || %[lt1] = [I0 ++ %[m_4]];\n\t"
+        "(%0, %1) = BYTEUNPACK r1:0;\n\t"
+        "A1 += %h1 * %h[fe], A0 += %h1 * %d[fe] (FU) || %[fex] = [I2 ++ %[m_4]] || %[lt0] = [I3 ++ %[m_4]];\n\t"
+        "A1 += %d1 * %h[fo], A0 += %d1 * %d[fo] (FU);\n\t"
+        "A1 += %h0 * %h[fex], A0 += %h0 * %d[fex] (FU) || %[fe] = [I2 ++ %[m_4]] || %[fo] = [I3 ++ %[m_4]];\n\t"
+        "A1 += %d0 * %h[lt0], A0 += %d0 * %d[lt0] (FU);\n\t"
+        "DISALGNEXCPT || %[lt0] = [I0++%[m_4]];\n\t"
+        "(%0, %1) = BYTEUNPACK r1:0 (R);\n\t"
+        "A1 += %h1 * %h[fe], A0 += %h1 * %d[fe] (FU) || %[fex] = [I2 ++ %[m_4]] || %[lt1] = [I3 ++ %[m_4]];\n\t"
+        "A1 += %d1 * %h[fo], A0 += %d1 * %d[fo] (FU);\n\t"
+        "A1 += %h0 * %h[fex], A0 += %h0 * %d[fex] (FU) || %[fe] = [I2 ++ %[m_4]] || %[fo] = [I3 ++ %[m_4]];\n\t"
+        "%0 = (A1 += %d0 * %h[lt1]), %1 = (A0 += %d0 * %d[lt1]) (FU);\n\t"
+        "LOOP_END inner_add;\n\t"
+        /*=========================*/
+        /* chop s1s */
+        "%[lt1] = %h2 (Z);\n\t"
+        "%2 >>= 16;\n\t"
+        "%[lt1] = %[lt1] - %2;\n\t"
+        "%2 <<= 4;\n\t"
+        "%2 = %2 + %[lt1];\n\t"
+        /* add s1s * 8 to s2 */
+        "%2 <<= 3;\n\t"
+        "%1 = %1 + %2;\n\t"
+        /* chop s2 */
+        "%2 = %h1 (Z);\n\t"
+        "%1 >>= 16;\n\t"
+        "%2 = %2 - %1;\n\t"
+        "%1 <<= 4;\n\t"
+        "%1 = %1 + %2;\n\t"
+        /* chop s1 */
+        "%2 = %h0 (Z);\n\t"
+        "%0 >>= 16;\n\t"
+        "%2 = %2 - %0;\n\t"
+        "%0 <<= 4;\n\t"
+        "%0 = %0 + %2;\n\t"
+        /* more then 8 byte left? */
+        "CC = %[len] <= %[pt] (IU);\n\t"
+        "IF !CC JUMP 1b;\n\t"
+        /****************************/
+        /* complete piece left? */
+        "CC = %[len] <= 3 (IU);\n\t"
+        "IF CC JUMP 3f;\n\t"
+        /* handle complete piece */
+        "%0 <<= 2;\n\t"
+        "%1 = %1 + %0;\n\t"
+        "A0 = %1;\n\t"
+        "%[len] += -4;\n\t"
+        "I2 += %[m_4];\n\t"
+        "I3 += %[m_4];\n\t"
+        "DISALGNEXCPT || %[lt1] = [I0 ++ %[m_4]];\n\t"
+        "(%0, %1) = BYTEUNPACK r1:0 || %[fe] = [I2 ++ %[m_4]] || %[fo] = [I3 ++ %[m_4]];\n\t"
+        "A1 += %h1 * %h[fe], A0 += %h1 * %d[fe] (FU) || %[fex] = [I2 ++ %[m_4]] || %[lt0] = [I3 ++ %[m_4]];\n\t"
+        "A1 += %d1 * %h[fo], A0 += %d1 * %d[fo] (FU);\n\t"
+        "A1 += %h0 * %h[fex], A0 += %h0 * %d[fex] (FU) || %[fe] = [I2 ++ %[m_4]] || %[fo] = [I3 ++ %[m_4]];\n\t"
+        "%0 = (A1 += %d0 * %h[lt0]), %1 = (A0 += %d0 * %d[lt0]) (FU);\n\t"
+        /* check remaining len */
+        "3:\n\t"
+        "CC = %[len] == 0;\n\t"
+        "IF CC JUMP 4f;\n\t"
+        /* handle trailer */
+        "%[pt] = I0;\n\t"
+        "%[pt] += -4;\n\t"
+        "LOOP trailer_add LC1 = %[len];\n\t"
+        /*=========================*/
+        "LOOP_BEGIN trailer_add;\n\t"
+        "%[lt0] = B [%[pt] ++] (Z);\n\t"
+        "%0 = %0 + %[lt0];\n\t"
+        "%1 = %1 + %0;\n\t"
+        "LOOP_END trailer_add;\n\t"
+        "%[len] = 0;\n\t"
+        "4:\n\t"
+        /* chop s1 */
+        /* chop s2 */
+        "%2 = %h1 (Z);\n\t"
+        "%8 = %h0 (Z);\n\t"
+        "%1 >>= 16;\n\t"
+        "%0 >>= 16;\n\t"
+        "%2 = %2 - %1;\n\t"
+        "%8 = %8 - %0;\n\t"
+        "%1 <<= 4;\n\t"
+        "%0 <<= 4;\n\t"
+        "%1 = %1 + %2;\n\t"
+        "%2 = %[base] (Z);\n\t"
+        "%0 = %0 + %8;\n\t"
+        "%9 = %1 - %2;\n\t"
+        "%8 = %0 - %2;\n\t"
+        "CC = %2 <= %1 (IU);\n\t"
+        "IF CC %1 = %9;\n\t"
+        "CC = %2 <= %0 (IU);\n\t"
+        "IF CC %0 = %8\n\t"
+        /* disable circular addressing again */
+        "L0 = %[len];\n\t"
+        "L2 = %[len];\n\t"
+        "L3 = %[len];\n\t"
+        : /* %0  */ "=q7" (s1),
+          /* %1  */ "=q6" (s2),
+          /* %2  */ "=d" (s1s),
+          /* %3  */ [lt0] "=q0" (lt0),
+          /* %4  */ [lt1] "=q1" (lt1),
+          /* %5  */ [len] "=a" (len),
+          /* %6  */ [lcount] "=a" (lcount),
+          /* %7  */ [pt] "=a" (pt),
+          /* %8  */ [fo] "=d" (fo),
+          /* %9  */ [fe] "=d" (fe),
+          /* %10 */ [fex] "=d" (fex)
+        : /*     */ [vnmax] "Kuh" (VNMAX),
+          /*     */ [base] "Kuh" (BASE),
+          /*     */ [m_4] "f" (4),
+          /*     */ [vord_o] "x" (&vord_o),
+          /*     */ [vord_e] "x" (&vord_e),
+          /*     */ [sorder] "i" (sizeof(vord_o)),
+          /*     */ "4" (buf),
+          /*     */ "0" (s1),
+          /*     */ "1" (s2),
+          /*     */ "5" (len)
+        : "CC", "LC1", "LB1", "LT1", "A0", "A1", "I0", "B0", "L0", "I2", "B2", "L2", "I3", "B3", "L3"
+    );
+
+    /* return recombined sums */
+    return (s2 << 16) | s1;
+}
+#endif
diff --git a/ia64/adler32.c b/ia64/adler32.c
new file mode 100644
index 0000000..d301733
--- /dev/null
+++ b/ia64/adler32.c
@@ -0,0 +1,650 @@
+/*
+ * adler32.c -- compute the Adler-32 checksum of a data stream
+ *   ia64 implementation
+ * Copyright (C) 1995-2007 Mark Adler
+ * Copyright (C) 2009-2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* @(#) $Id$ */
+
+/* inline asm ahead, GCC only */
+#include <limits.h>
+#ifdef __GNUC__
+#  define HAVE_ADLER32_VEC
+#  define MIN_WORK 64
+
+#  define VNMAX (7*NMAX)
+#  define SOULL (sizeof(unsigned long long))
+
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    union scale_order { unsigned short x[4][4]; unsigned long long d[4];};
+    local const union scale_order ord_le = {{{8,7,6,5},{4,3,2,1},{16,15,14,13},{12,11,10,9}}};
+// TODO: Big Endian is untested
+    local const union scale_order ord_be = {{{1,2,3,4},{5,6,7,8},{ 9,10,11,12},{13,14,15,16}}};
+    unsigned int s1, s2;
+
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    if (likely(len >= 4*SOULL)) {
+        const union scale_order *scale_order;
+        unsigned long long vs1, vs2;
+        unsigned long long in8;
+        unsigned int f, n, k;
+
+        if (!host_is_bigendian())
+            scale_order = &ord_le;
+        else
+            scale_order = &ord_be;
+
+        /* align hard down */
+        f = (unsigned int) ALIGN_DOWN_DIFF(buf, SOULL);
+        buf = (const Bytef *)ALIGN_DOWN(buf, SOULL);
+        n = SOULL - f;
+
+        /* add n times s1 to s2 for start round */
+        s2 += s1 * n;
+
+        k = len < VNMAX ? len : VNMAX;
+        len -= k;
+
+        /* insert scalar start somewhere */
+        vs1 = s1;
+        vs2 = s2;
+        {
+            unsigned long long a, b, c, x, y;
+
+            /* get input data */
+            in8 = *(const unsigned long long *)buf;
+            if (f) {
+                if (!host_is_bigendian()) {
+                    in8 >>= f * CHAR_BIT;
+                    in8 <<= f * CHAR_BIT;
+                } else {
+                    in8 <<= f * CHAR_BIT;
+                    in8 >>= f * CHAR_BIT;
+                }
+            }
+
+            asm (
+                /* add 8 byte horizontal and add to old qword */
+                "psad1	%3=r0, %7\n\t"
+                "unpack1.l	%5=r0, %7;;\n\t"
+
+                /* apply order, widen to 32 bit */
+                "add	%0=%0, %3\n\t"
+                "unpack1.h	%6=r0, %7\n\t"
+                "pmpy2.r	%2=%5, %9;;\n\t"
+
+                "add	%1=%1, %2\n\t"
+                "pmpy2.r	%4=%6, %8\n\t"
+                "pmpy2.l	%3=%5, %9;;\n\t"
+
+                "add	%2=%3, %4\n\t"
+                "pmpy2.l	%3=%6, %8;;\n\t"
+                "add	%1=%1, %3\n\t"
+                : /* %0 */ "=&r" (vs1),
+                  /* %1 */ "=&r" (vs2),
+                  /* %2 */ "=&r" (a),
+                  /* %3 */ "=&r" (b),
+                  /* %4 */ "=&r" (c),
+                  /* %5 */ "=&r" (x),
+                  /* %6 */ "=&r" (y)
+                : /* %7 */ "r" (in8),
+                  /* %8 */ "r" (scale_order->d[1]),
+                  /* %9 */ "r" (scale_order->d[0]),
+                  /*  */ "0" (vs1),
+                  /*  */ "1" (vs2)
+            );
+            vs2 += a;
+        }
+
+        buf += SOULL;
+        k -= n;
+
+        if (likely(k >= 2*SOULL)) {
+            register const union scale_order *l_ord asm ("r15");
+            register const Bytef *l_buf asm ("r16");
+            register unsigned int l_k asm ("r17");
+            register unsigned int l_len asm ("r18");
+            register unsigned long long l_vs1 asm ("r19");
+            register unsigned long long l_vs2 asm ("r20");
+            register unsigned long long l_frame asm ("r21");
+            void *br_reg;
+
+            asm (
+                ".equ inner_loop_count, 31*64\n\t" /* 127*16/64 == 31.75 */
+                ".equ buf_ptr_1, %0\n\t"
+                ".equ k, %1\n\t"
+                ".equ len, %2\n\t"
+                ".equ order_ptr, %3\n\t"
+                ".equ vs1, %4\n\t"
+                ".equ vs2, %5\n\t"
+                ".equ vs1_l, vs1\n\t"
+
+                ".equ old_frame_state, r34\n\t"
+                ".equ buf_ptr_2, r35\n\t"
+                ".equ buf_ptr_3, r36\n\t"
+                ".equ buf_ptr_4, r37\n\t"
+                ".equ loop_rem, r38\n\t"
+                ".equ order_hihi, r39\n\t"
+                ".equ order_hilo, r40\n\t"
+                ".equ order_lohi, r41\n\t"
+                ".equ order_lolo, r42\n\t"
+
+                ".equ input8_1, r43\n\t"
+                ".equ input8_2, r44\n\t"
+                ".equ input8_3, r45\n\t"
+                ".equ input8_4, r46\n\t"
+                ".equ input8_5, r47\n\t"
+                ".equ input8_6, r48\n\t"
+                ".equ input8_7, r49\n\t"
+                ".equ input8_8, r50\n\t"
+
+                ".equ sad_1, r51\n\t"
+                ".equ sad_2, r52\n\t"
+                ".equ sad_3, r53\n\t"
+                ".equ sad_4, r54\n\t"
+                ".equ sad_5, r55\n\t"
+                ".equ sad_6, r56\n\t"
+                ".equ sad_7, r57\n\t"
+                ".equ sad_8, r58\n\t"
+
+                ".equ vs1_rsum_l, r59\n\t"
+                ".equ vs1_rsum_h, r60\n\t"
+                ".equ vs1_h, r61\n\t"
+                ".equ vs2_wsum_ll, r62\n\t"
+                ".equ vs2_wsum_lh, r63\n\t"
+                ".equ vs2_wsum_hl, r64\n\t"
+                ".equ vs2_wsum_hh, r65\n\t"
+
+                ".equ unpck_01, r66\n\t"
+                ".equ unpck_02, r67\n\t"
+                ".equ unpck_03, r68\n\t"
+                ".equ unpck_04, r69\n\t"
+                ".equ unpck_05, r70\n\t"
+                ".equ unpck_06, r71\n\t"
+                ".equ unpck_07, r72\n\t"
+                ".equ unpck_08, r73\n\t"
+                ".equ unpck_09, r74\n\t"
+                ".equ unpck_10, r75\n\t"
+                ".equ unpck_11, r76\n\t"
+                ".equ unpck_12, r77\n\t"
+                ".equ unpck_13, r78\n\t"
+                ".equ unpck_14, r79\n\t"
+                ".equ unpck_15, r80\n\t"
+                ".equ unpck_16, r81\n\t"
+
+                "mov	%6 = ar.pfs\n\t"
+                /* force creation of a new register frame */
+                "br.call.sptk.many %7 = 8f\n\t"
+
+                "nop.m 0;;\n\t"
+                "mov	ar.pfs = %6\n\t"
+
+                ".subsection 2\n"
+                "8:\n\t"
+                "alloc	old_frame_state = ar.pfs, 3, 47, 0, 0;;\n\t"
+                "ld8	order_hilo = [order_ptr], 8\n\t"
+                "nop.i 0;;\n\t"
+
+                "ld8	order_hihi = [order_ptr], 8;;\n\t"
+                "ld8	order_lolo = [order_ptr], 8\n\t"
+                "nop.i 0;;\n\t"
+
+                "ld8	order_lohi = [order_ptr], 8\n\t"
+                "mov	vs1_h = r0;;\n\t"
+                "nop.i 0\n\t"
+
+                "nop.m 0\n\t"
+                "mov	vs1_rsum_l = r0\n\t"
+                "mov	vs1_rsum_h = r0\n\t"
+
+                "4:\n\t"
+                "add	buf_ptr_2 = 8, buf_ptr_1\n\t"
+                "add	buf_ptr_3 = 16, buf_ptr_1\n\t"
+                "add	buf_ptr_4 = 24, buf_ptr_1\n\t"
+
+                "ld8	input8_1 = [buf_ptr_1], 32\n\t"
+                "mov	unpck_13 = inner_loop_count\n\t"
+                "mov	vs2_wsum_ll = r0;;\n\t"
+
+                "ld8	input8_2 = [buf_ptr_2], 32\n\t"
+                "cmp4.ltu	p7,p6 = k, unpck_13\n\t"
+                "mov	vs2_wsum_lh = r0\n\t"
+
+                "mov	vs2_wsum_hl = r0;;\n\t"
+                "(p6) mov	unpck_15 = inner_loop_count\n\t"
+                "(p7) mov	unpck_15 = k;;\n\t"
+
+                "mov	unpck_16 = buf_ptr_1\n\t"
+                "and	unpck_14 = -16, unpck_15\n\t"
+                "and	loop_rem = 63, unpck_15;;\n\t"
+
+                "mov	vs2_wsum_hh = r0\n\t"
+                "shr.u	unpck_13 = unpck_15, 6\n\t"
+                "add	unpck_16 = unpck_15, unpck_16;;\n\t"
+
+                "cmp4.eq	p7,p6 = 0, unpck_13\n\t"
+                "sub	k = k, unpck_14\n\t"
+                "add	unpck_16 = -33, unpck_16;;\n\t"
+
+                /* prefetch end of buffer for this iterration,
+                 * to make sure OS sees possible page faults so
+                 * OS can swap page in, because speculative
+                 * loads will not.
+                 */
+                "lfetch.fault	[unpck_16]\n\t"
+                "nop.i	1\n\t"
+                "(p7) br.cond.dpnt.few 5f\n\t"
+
+                "ld8	input8_3 = [buf_ptr_3], 32\n\t"
+                "mov	unpck_01 = r0\n\t"
+                "add	unpck_13 = -1, unpck_13\n\t"
+
+                "ld8	input8_4 = [buf_ptr_4], 32\n\t"
+                "mov	unpck_09 = r0\n\t"
+                "mov	unpck_02 = r0\n\t"
+
+                "mov	unpck_10 = r0\n\t"
+                "mov	unpck_03 = r0\n\t"
+                "mov	unpck_11 = r0\n\t"
+
+                "mov	unpck_04 = r0\n\t"
+                "mov	unpck_12 = r0;;\n\t"
+                "mov.i	ar.lc = unpck_13\n"
+
+                "1:\n\t"
+                "ld8	input8_5 = [buf_ptr_1], 32\n\t"
+                "padd2	unpck_01 = unpck_01, unpck_09\n\t"
+                "psad1	sad_1 = r0, input8_1\n\t"
+
+                "ld8	input8_6 = [buf_ptr_2], 32\n\t"
+                "padd2	unpck_02 = unpck_02, unpck_10\n\t"
+                "psad1	sad_2 = r0, input8_2\n\t"
+
+                "ld8	input8_7 = [buf_ptr_3], 32\n\t"
+                "padd2	unpck_03 = unpck_03, unpck_11\n\t"
+                "psad1	sad_3 = r0, input8_3\n\t"
+
+                "ld8	input8_8 = [buf_ptr_4], 32\n\t"
+                "padd2	unpck_04 = unpck_04, unpck_12\n\t"
+                "psad1	sad_4 = r0, input8_4\n\t"
+
+                "padd4	vs1_rsum_l = vs1_rsum_l, vs1_l\n\t"
+                "padd4	vs1_rsum_h = vs1_rsum_h, vs1_h;;\n\t"
+                "psad1	sad_5 = r0, input8_5\n\t"
+
+                "padd2	vs2_wsum_ll = vs2_wsum_ll, unpck_01\n\t"
+                "padd2	vs2_wsum_lh = vs2_wsum_lh, unpck_02\n\t"
+                "unpack1.l	unpck_05 = r0, input8_3\n\t"
+
+                "padd4	vs1_l = vs1_l, sad_1\n\t"
+                "psad1	sad_6 = r0, input8_6\n\t"
+                "unpack1.h	unpck_06 = r0, input8_3\n\t"
+
+                "padd4	vs1_h = vs1_h, sad_2\n\t"
+                "unpack1.l	unpck_01 = r0, input8_1\n\t"
+                "unpack1.h	unpck_02 = r0, input8_1\n\t"
+
+                "padd2	vs2_wsum_hl = vs2_wsum_hl, unpck_03\n\t"
+                "unpack1.l	unpck_07 = r0, input8_4\n\t"
+                "unpack1.h	unpck_08 = r0, input8_4\n\t"
+
+                "padd2	vs2_wsum_hh = vs2_wsum_hh, unpck_04\n\t"
+                "unpack1.l	unpck_09 = r0, input8_5\n\t"
+                "unpack1.h	unpck_10 = r0, input8_5;;\n\t"
+
+                "padd4	vs1_rsum_l = vs1_rsum_l, vs1_l\n\t"
+                "unpack1.l	unpck_03 = r0, input8_2\n\t"
+                "unpack1.h	unpck_04 = r0, input8_2\n\t"
+
+                "padd4	vs1_rsum_h = vs1_rsum_h, vs1_h\n\t"
+                "unpack1.l	unpck_11 = r0, input8_6\n\t"
+                "unpack1.h	unpck_12 = r0, input8_6\n\t"
+
+                "padd2	unpck_01 = unpck_01, unpck_05\n\t"
+                "unpack1.l	unpck_13 = r0, input8_7;;\n\t"
+                "mux2	vs1_rsum_h = vs1_rsum_h, 0x4e\n\t"
+
+                "padd2	unpck_02 = unpck_02, unpck_06\n\t"
+                "mux2	vs1_rsum_l = vs1_rsum_l, 0x4e\n\t"
+                "unpack1.h	unpck_14 = r0, input8_7\n\t"
+
+                "padd4	vs1_l = vs1_l, sad_3\n\t"
+                "unpack1.h	unpck_16 = r0, input8_8\n\t"
+                "unpack1.l	unpck_15 = r0, input8_8\n\t"
+
+                "padd2	unpck_03 = unpck_03, unpck_07\n\t"
+                "padd4	vs1_h = vs1_h, sad_4\n\t"
+                "psad1	sad_7 = r0, input8_7\n\t"
+
+                "ld8.s	input8_1 = [buf_ptr_1], 32\n\t"
+                "padd2	unpck_04 = unpck_04, unpck_08\n\t"
+                "psad1	sad_8 = r0, input8_8\n\t"
+
+                "ld8.s	input8_2 = [buf_ptr_2], 32\n\t"
+                "padd2	unpck_09 = unpck_09, unpck_13;;\n\t"
+                "padd2	unpck_10 = unpck_10, unpck_14\n\t"
+
+                "ld8.s	input8_3 = [buf_ptr_3], 32\n\t"
+                "padd4	vs1_rsum_l = vs1_rsum_l, vs1_l\n\t"
+                "padd4	vs1_rsum_h = vs1_rsum_h, vs1_h\n\t"
+
+                "ld8.s	input8_4 = [buf_ptr_4], 32\n\t"
+                "padd4	vs1_l = vs1_l, sad_5\n\t"
+                "padd4	vs1_h = vs1_h, sad_6;;\n\t"
+
+                "padd4	vs1_rsum_l = vs1_rsum_l, vs1_l\n\t"
+                "padd4	vs1_rsum_h = vs1_rsum_h, vs1_h\n\t"
+                "padd4	vs1_l = vs1_l, sad_7\n\t"
+
+                "padd4	vs1_h = vs1_h, sad_8\n\t"
+                "padd2	unpck_11 = unpck_11, unpck_15\n\t"
+                "padd2	unpck_12 = unpck_12, unpck_16\n\t"
+
+                "nop.m 0\n\t"
+                "nop.i 0\n\t"
+                "br.cloop.sptk.many 1b\n\t"
+
+                "padd2	unpck_01 = unpck_01, unpck_09\n\t"
+                "padd2	unpck_02 = unpck_02, unpck_10;;\n\t"
+                "padd2	unpck_03 = unpck_03, unpck_11\n\t"
+
+                "padd2	unpck_04 = unpck_04, unpck_12\n\t"
+                "padd2	vs2_wsum_ll = vs2_wsum_ll, unpck_01\n\t"
+                "padd2	vs2_wsum_lh = vs2_wsum_lh, unpck_02;;\n\t"
+
+                "padd2	vs2_wsum_hl = vs2_wsum_hl, unpck_03\n\t"
+                "padd2	vs2_wsum_hh = vs2_wsum_hh, unpck_04\n\t"
+                "nop.i 0\n\t"
+
+                "5:\n\t"
+                "cmp4.gtu	p7,p6 = 16, loop_rem\n\t"
+                "add	buf_ptr_1 = -16, buf_ptr_1\n\t"
+                "add	buf_ptr_2 = -16, buf_ptr_2;;\n\t"
+
+                "nop.m 0\n\t"
+                "nop.i 0\n\t"
+                "(p7) br.cond.dptk.few 2f\n"
+
+                "add	loop_rem = -16, loop_rem;;\n\t"
+                "nop.m 0\n\t"
+                "shr.u	unpck_15 = loop_rem, 4;;\n\t"
+
+                "nop.m 0\n\t"
+                "mov.i ar.lc = unpck_15\n\t"
+                "nop.i 0\n\t"
+
+                "3:\n\t"
+                "padd4	vs1_rsum_l = vs1_rsum_l, vs1_l\n\t"
+                "psad1	sad_1 = r0, input8_1;;\n\t"
+                "psad1	sad_2 = r0, input8_2\n\t"
+
+                "padd4	vs1_rsum_h = vs1_rsum_h, vs1_h\n\t"
+                "unpack1.l	unpck_01 = r0, input8_1;;\n\t"
+                "mux2	vs1_rsum_l = vs1_rsum_l, 0x4e\n\t"
+
+                "padd4 vs1_l = vs1_l, sad_1\n\t"
+                "unpack1.h	unpck_02 = r0, input8_1\n\t"
+                "mux2	vs1_rsum_h = vs1_rsum_h, 0x4e\n\t"
+
+                "padd4 vs1_h = vs1_h, sad_2\n\t"
+                "unpack1.l	unpck_03 = r0, input8_2\n\t"
+                "unpack1.h	unpck_04 = r0, input8_2\n\t"
+
+                "ld8.s input8_1 = [buf_ptr_1], 16\n\t"
+                "padd2	vs2_wsum_ll = vs2_wsum_ll, unpck_01;;\n\t"
+                "padd2	vs2_wsum_lh = vs2_wsum_lh, unpck_02\n\t"
+
+                "ld8.s input8_2 = [buf_ptr_2], 16\n\t"
+                "padd2	vs2_wsum_hl = vs2_wsum_hl, unpck_03\n\t"
+                "padd2	vs2_wsum_hh = vs2_wsum_hh, unpck_04\n\t"
+
+                "nop.m 0\n\t"
+                "nop.i 0\n\t"
+                "br.cloop.sptk.few 3b\n"
+
+                "2:\n\t"
+                "add	buf_ptr_1 = -16, buf_ptr_1\n\t"
+                "pmpy2.r	unpck_01 = vs2_wsum_ll, order_lolo\n\t"
+                "pmpy2.l	unpck_02 = vs2_wsum_ll, order_lolo;;\n\t"
+
+                "add	buf_ptr_2 = -16, buf_ptr_2\n\t"
+                "pmpy2.r	unpck_03 = vs2_wsum_lh, order_lohi\n\t"
+                "pmpy2.l	unpck_04 = vs2_wsum_lh, order_lohi\n\t"
+
+                "padd4	unpck_01 = unpck_01, unpck_02\n\t"
+                "pmpy2.r	unpck_05 = vs2_wsum_hl, order_hilo\n\t"
+                "pmpy2.l	unpck_06 = vs2_wsum_hl, order_hilo;;\n\t"
+
+                "padd4	unpck_03 = unpck_03, unpck_04\n\t"
+                "pmpy2.r	unpck_07 = vs2_wsum_hh, order_hihi\n\t"
+                "pmpy2.l	unpck_08 = vs2_wsum_hh, order_hihi\n\t"
+
+                "padd4	unpck_05 = unpck_05, unpck_06;;\n\t"
+                "padd4	unpck_07 = unpck_07, unpck_08\n\t"
+                "padd4	unpck_01 = unpck_01, unpck_03;;\n\t"
+
+                "padd4	unpck_05 = unpck_05, unpck_07\n\t"
+                "padd4	vs2 = vs2, unpck_01;;\n\t"
+                "padd4	vs2 = vs2, unpck_05\n\t"
+
+                "cmp4.ltu	p7,p6 = 16, k\n\t"
+                "nop.m 0\n\t"
+                "nop.i 0\n\t;;"
+
+                "nop.m 0\n\t"
+                "nop.i 0\n\t"
+                "(p7) br.cond.dptk.few 4b\n\t"
+
+                "padd4	vs1_l = vs1_l, vs1_h\n\t"
+                "mix2.r	unpck_01 = r0, vs1_rsum_l\n\t"
+                "mix2.l	unpck_02 = r0, vs1_rsum_l;;\n\t"
+
+                "mov	vs1_h = r0\n\t"
+                "mix2.r	unpck_03 = r0, vs1_rsum_h\n\t"
+                "mix2.l	unpck_04 = r0, vs1_rsum_h;;\n\t"
+
+                "nop.m 0\n\t"
+                "mix2.r	unpck_05 = r0, vs1_l\n\t"
+                "mix2.l	unpck_06 = r0, vs1_l\n\t"
+
+                "psub4	unpck_01 = unpck_01, unpck_02\n\t"
+                "psub4	unpck_03 = unpck_03, unpck_04\n\t"
+                "shl	unpck_02 = unpck_02, 4;;\n\t"
+
+                "padd4	vs1_rsum_l = unpck_01, unpck_02\n\t"
+                "psub4	unpck_05 = unpck_05, unpck_06\n\t"
+                "shl	unpck_04 = unpck_04, 4;;\n\t"
+
+                "padd4	vs1_rsum_h = unpck_03, unpck_04\n\t"
+                "shl	unpck_06 = unpck_06, 4;;\n\t"
+
+                "padd4	vs1_l = unpck_05, unpck_06\n\t"
+                "padd4	vs1_rsum_l = vs1_rsum_l, vs1_rsum_h;;\n\t"
+                "shl	vs1_rsum_l = vs1_rsum_l, 4;;\n\t"
+
+                "padd4	vs2 = vs2, vs1_rsum_l\n\t"
+                "add	len = len, k;;\n\t"
+                "mix2.r	unpck_01 = r0, vs2\n\t"
+
+                "mov	vs1_rsum_l = r0\n\t"
+                "mov	unpck_15 = %8\n\t"
+                "mix2.l	unpck_02 = r0, vs2;;\n\t"
+
+                "psub4	unpck_01 = unpck_01, unpck_02\n\t"
+                "shl	unpck_02 = unpck_02, 4;;\n\t"
+                "padd4	vs2 = unpck_01, unpck_02\n\t"
+
+                "cmp4.ltu	p7,p6 = len, unpck_15;;\n\t"
+                "(p6) mov k = %8\n\t"
+                "(p7) mov k = len;;\n\t"
+
+                "sub	len = len, k\n\t"
+                "cmp4.ltu	p7,p6 = 16, k\n\t"
+                "mov	vs1_rsum_h = r0;;\n\t"
+
+                "nop.m 0\n\t"
+                "nop.i 0\n\t"
+                "(p7) br.cond.dptk 4b\n\t"
+
+                "nop.m 0\n\t"
+                "mov	ar.pfs = old_frame_state;;\n\t"
+                "nop.i 0\n\t"
+
+                "nop.m 0\n\t"
+                "nop.i 0\n\t"
+                "br.ret.sptk.many %7\n\t"
+
+                "nop.m 22\n"
+                "nop.i 23\n"
+                "nop.i 24\n"
+                /*
+                 * ??? .previous seems to be buggy. It
+                 * removes the last bundle from this
+                 * section and inserts it into the previous...
+                 */
+                ".previous\n\t"
+                : /* %0 */ "=r" (l_buf),
+                  /* %1 */ "=r" (l_k),
+                  /* %2 */ "=r" (l_len),
+                  /* %3 */ "=r" (l_ord),
+                  /* %4 */ "=r" (l_vs1),
+                  /* %5 */ "=r" (l_vs2),
+                  /* %6 */ "=r" (l_frame),
+                  /* %7 */ "=b" (br_reg)
+                : /* %8 */ "i" (VNMAX),
+                  /*  */ "0" (buf),
+                  /*  */ "1" (k),
+                  /*  */ "2" (len),
+                  /*  */ "3" (scale_order),
+                  /*  */ "4" (vs1),
+                  /*  */ "5" (vs2)
+                : "p6", "p7", "ar.lc"
+            );
+            buf = l_buf;
+            k = l_k;
+            len = l_len;
+            vs1 = l_vs1;
+            vs2 = l_vs2;
+        }
+
+        /* complete piece left? */
+        if (k >= SOULL) {
+            unsigned long long a, b, c, x, y;
+
+            in8 = *(const unsigned long long *)buf;
+            asm (
+                "shladd	%1=%0,3,%1\n\t"
+                "psad1	%3=r0, %7\n\t"
+                "unpack1.l	%5=r0, %7;;\n\t"
+
+                "add	%0=%0, %3\n\t"
+                "unpack1.h	%6=r0, %7\n\t"
+                "pmpy2.r	%2=%5, %9;;\n\t"
+
+                "add	%1=%1, %2\n\t"
+                "pmpy2.r	%4=%6, %8\n\t"
+                "pmpy2.l	%3=%5, %9;;\n\t"
+
+                "add	%2=%3, %4\n\t"
+                "pmpy2.l	%3=%6, %8;;\n\t"
+                "add	%1=%1, %3\n\t"
+                : /* %0 */ "=&r" (vs1),
+                  /* %1 */ "=&r" (vs2),
+                  /* %2 */ "=&r" (a),
+                  /* %3 */ "=&r" (b),
+                  /* %4 */ "=&r" (c),
+                  /* %5 */ "=&r" (x),
+                  /* %6 */ "=&r" (y)
+                : /* %7 */ "r" (in8),
+                  /* %8 */ "r" (scale_order->d[1]),
+                  /* %9 */ "r" (scale_order->d[0]),
+                  /*  */ "0" (vs1),
+                  /*  */ "1" (vs2)
+            );
+            vs2 += a;
+            buf += SOULL;
+            k   -= SOULL;
+        }
+
+        if (likely(k)) {
+            unsigned long long a, b, c, x, y;
+            /* get input data */
+            in8 = *(const unsigned long long *)buf;
+            unsigned int n = SOULL - k;
+
+            /* swizzle data in place */
+            if (!host_is_bigendian())
+                in8 <<= n * CHAR_BIT;
+            else
+                in8 >>= n * CHAR_BIT;
+
+            /* add k times vs1 for this trailer */
+            vs2 += vs1 * k;
+
+            asm (
+                /* add 8 byte horizontal and add to old qword */
+                "psad1	%3=r0, %7\n\t"
+                "unpack1.l	%5=r0, %7;;\n\t"
+
+                /* apply order, widen to 32 bit */
+                "add	%0=%0, %3\n\t"
+                "unpack1.h	%6=r0, %7\n\t"
+                "pmpy2.r	%2=%5, %9;;\n\t"
+
+                "add	%1=%1, %2\n\t"
+                "pmpy2.r	%4=%6, %8\n\t"
+                "pmpy2.l	%3=%5, %9;;\n\t"
+
+                "add	%2=%3, %4\n\t"
+                "pmpy2.l	%3=%6, %8;;\n\t"
+                "add	%1=%1, %3\n\t"
+                : /* %0 */ "=&r" (vs1),
+                  /* %1 */ "=&r" (vs2),
+                  /* %2 */ "=&r" (a),
+                  /* %3 */ "=&r" (b),
+                  /* %4 */ "=&r" (c),
+                  /* %5 */ "=&r" (x),
+                  /* %6 */ "=&r" (y)
+                : /* %7 */ "r" (in8),
+                  /* %8 */ "r" (scale_order->d[1]),
+                  /* %9 */ "r" (scale_order->d[0]),
+                  /*  */ "0" (vs1),
+                  /*  */ "1" (vs2)
+            );
+            vs2 += a;
+
+            buf += k;
+            k -= k;
+        }
+
+        /* vs1 is one giant 64 bit sum, but we only calc to 32 bit */
+        s1 = vs1;
+        /* add both vs2 sums */
+        asm (
+            "unpack4.h	%0=r0, %1\n\t"
+            "unpack4.l	%1=r0, %1\n\t"
+            : /* %0 */ "=r" (s2),
+              /* %1 */ "=r" (vs2)
+            : /* %2 */ "1" (vs2)
+        );
+        s2 += vs2;
+        /* modulo again in scalar code */
+    }
+
+    if (unlikely(len)) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--len);
+    MOD28(s1);
+    MOD28(s2);
+    return (s2 << 16) | s1;
+}
+#endif
diff --git a/mips/adler32.c b/mips/adler32.c
new file mode 100644
index 0000000..4f17c71
--- /dev/null
+++ b/mips/adler32.c
@@ -0,0 +1,521 @@
+/*
+ * adler32.c -- compute the Adler-32 checksum of a data stream
+ *   mips implementation
+ * Copyright (C) 1995-2004 Mark Adler
+ * Copyright (C) 2009-2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* @(#) $Id$ */
+
+/* we use a bunch of inline asm and GCC vector internals, so GCC it is */
+#ifdef __GNUC__
+#  if _MIPS_SZPTR < 64
+#    define SZPRFX
+#  else
+#    define SZPRFX "d"
+#  endif
+#  include <limits.h>
+#  ifdef __mips_loongson_vector_rev
+#    define HAVE_ADLER32_VEC
+#    define MIN_WORK 64
+
+/*
+ * The loongson vector stuff is basically a variant of MMX,
+ * looks like ST had some licenses for that from their x86 days
+ */
+#    include <loongson.h>
+#    define SOV8 (sizeof(uint8x8_t))
+#    define VNMAX (4*NMAX)
+
+/* GCCs loongson port looks like a quick hack.
+ * It can output some simple vector instruction sequences,
+ * but creates horrible stuff when you really use it.
+ * So more coding by hand...
+ */
+
+/* ========================================================================= */
+local inline uint32x2_t vector_chop(uint32x2_t x)
+{
+    uint32x2_t y;
+
+    y = psllw_u(x, 16);
+    y = psrlw_u(y, 16);
+    x = psrlw_u(x, 16);
+    y = psubw_u(y, x);
+    x = psllw_u(x, 4);
+    x = paddw_u(x, y);
+    return x;
+}
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1, s2;
+
+    /* split Adler-32 into component sums */
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    if(likely(len >= 2*SOV8))
+    {
+        /* Loongsons and their ST MMX foo are little endian */
+        static const int16x4_t vord_lo = {8,7,6,5};
+        static const int16x4_t vord_hi = {4,3,2,1};
+        uint32x2_t vs2, vs1;
+        int16x4_t in_lo, in_hi;
+        uint8x8_t v0 = {0};
+        uint8x8_t in8;
+        unsigned f, n;
+        unsigned k;
+
+        /*
+         * Add stuff to achieve alignment
+         */
+        /* align hard down */
+        f = (unsigned) ALIGN_DOWN_DIFF(buf, SOV8);
+        n = SOV8 - f;
+        buf = (const unsigned char *)ALIGN_DOWN(buf, SOV8);
+
+        /* add n times s1 to s2 for start round */
+        s2 += s1 * n;
+
+        k = len < VNMAX ? (unsigned)len : VNMAX;
+        len -= k;
+
+        /* insert scalar start somewhere */
+        vs1 = (uint32x2_t)(unsigned long long)s1;
+        vs2 = (uint32x2_t)(unsigned long long)s2;
+
+        /* get input data */
+        /* add all byte horizontal and add to old qword */
+        /* apply order, add 4 byte horizontal and add to old dword */
+        asm (
+                "ldc1	%0, %9\n\t"
+                "dsrl	%0, %0, %5\n\t"
+                "dsll	%0, %0, %5\n\t"
+                "biadd	%4, %0\n\t"
+                "punpcklbh	%3, %0, %6\n\t"
+                "paddw	%1, %1, %4\n\t"
+                "punpckhbh	%4, %0, %6\n\t"
+                "pmaddhw	%3, %3, %7\n\t"
+                "pmaddhw	%4, %4, %8\n\t"
+                "paddw	%2, %2, %3\n\t"
+                "paddw	%2, %2, %4\n\t"
+                : /* %0  */ "=&f" (in8),
+                  /* %1  */ "=f" (vs1),
+                  /* %2  */ "=f" (vs2),
+                  /* %3  */ "=&f" (in_lo),
+                  /* %4  */ "=&f" (in_hi)
+                : /* %5  */ "f" (f * CHAR_BIT),
+                  /* %6  */ "f" (v0),
+                  /* %7  */ "f" (vord_lo),
+                  /* %8  */ "f" (vord_hi),
+                  /* %9  */ "m" (*buf),
+                  /* %10 */ "1" (vs1),
+                  /* %11 */ "2" (vs2)
+        );
+        buf += SOV8;
+        k -= n;
+
+        if (likely(k >= SOV8)) do {
+            uint32x2_t vs1_r;
+            uint16x4_t vs2_lo, vs2_hi;
+            int t, j;
+
+            /*
+             * GCC generates horible loop code, so write
+             * the core loop by hand...
+             */
+            __asm__ __volatile__ (
+                    ".set noreorder\n\t"
+                    "b	5f\n\t"
+                    "xor	%3, %3, %3\n"
+                    "2:\n\t"
+                    "xor	%6, %6, %6\n\t"
+                    "sll	%10, %11, 3\n\t"
+                    "xor	%7, %7, %7\n\t"
+                    "subu	%9, %9, %10\n"
+                    "1:\n\t"
+                    "ldc1	%0, (%8)\n\t"
+                    SZPRFX"addiu	%8, %8, 8\n\t"
+                    "addiu	%11, %11, -1\n\t"
+                    "paddw	%3, %1, %3\n\t"
+                    "biadd	%5, %0\n\t"
+                    "punpcklbh	%4, %0, %12\n\t"
+                    "paddw	%1, %5, %1\n\t"
+                    "punpckhbh	%5, %0, %12\n\t"
+                    "paddh	%6, %6, %4\n\t"
+                    "bnez	%11, 1b\n\t"
+                    "paddh	%7, %7, %5\n\t"
+                    /* loop bottom  */
+                    "pshufh	%1, %1, %15\n\t"
+                    "sltiu	%10, %9, 8\n\t"
+                    "pmaddhw	%4, %6, %13\n\t"
+                    "pmaddhw	%5, %7, %14\n\t"
+                    "paddw	%2, %2, %4\n\t"
+                    "bnez	%10, 4f\n\t"
+                    "paddw	%2, %2, %5\n"
+                    "5:\n\t"
+                    "sltiu	%10, %9, 1032\n\t"
+                    "beqz	%10, 2b\n\t"
+                    "li	%11, 128\n\t"
+                    "b	2b\n\t"
+                    "srl	%11, %9, 3\n"
+                    "4:\n\t"
+                    ".set reorder\n\t"
+                    : /* %0  */ "=&f" (in8),
+                      /* %1  */ "=f" (vs1),
+                      /* %2  */ "=f" (vs2),
+                      /* %3  */ "=&f" (vs1_r),
+                      /* %4  */ "=&f" (in_lo),
+                      /* %5  */ "=&f" (in_hi),
+                      /* %6  */ "=&f" (vs2_lo),
+                      /* %7  */ "=&f" (vs2_hi),
+                      /* %8  */ "=d" (buf),
+                      /* %9  */ "=r" (k),
+                      /* %10 */ "=r" (t),
+                      /* %11 */ "=r" (j)
+                    : /* %12 */ "f" (v0),
+                      /* %13 */ "f" (vord_lo),
+                      /* %14 */ "f" (vord_hi),
+                      /* %15 */ "f" (0x4e),
+                      /* %15 */ "1" (vs1),
+                      /* %16 */ "2" (vs2),
+                      /* %17 */ "8" (buf),
+                      /* %18 */ "9" (k)
+            );
+            /* And the rest of the generated code also looks awful.
+             * Looks like GCC is missing instruction patterns for:
+             * - 64 bit shifts in loongson copro regs
+             * - logic in loongson copro regs
+             * and to make things much worse, GCC seems to be missing
+             * a loongson copro register <-> copro register move
+             * pattern (for example using an or instruction), instead
+             * GCC always moves over the GPR.
+             *
+             * But still, let the compiler handle this, we get some
+             * extra moves between copro regs and GPR, but save us
+             * a lot of work.
+             * And maybe some day some one will fix this...
+             */
+
+            /* chop vs1 round sum before multiplying by 8 */
+            vs1_r = vector_chop(vs1_r);
+            /* add all vs1 for 8 times */
+            vs2 = paddw_u(psllw_u(vs1_r, 3), vs2);
+            /* chop the vectors to something in the range of BASE */
+            vs2 = vector_chop(vs2);
+            vs1 = vector_chop(vs1);
+            len += k;
+            k = len < VNMAX ? (unsigned)len : VNMAX;
+            len -= k;
+        } while (likely(k >= SOV8));
+
+        if (likely(k)) {
+            uint32x2_t vk;
+            /* handle trailer */
+            f = SOV8 - k;
+
+            vk = (uint32x2_t)(unsigned long long)k;
+
+            /* get input data */
+            /* add all byte horizontal and add to old qword */
+            /* add k times vs1 for this trailer */
+            /* apply order, add 4 byte horizontal and add to old dword */
+            __asm__ (
+                    "ldc1	%0, %11\n\t"
+                    "pmuluw	%3, %1, %6\n\t"
+                    "pshufh	%1, %1, %10\n\t"
+                    "paddw	%2, %2, %3\n\t"
+                    "pmuluw	%3, %1, %6\n\t"
+                    "dsll	%0, %0, %5\n\t"
+                    "biadd	%4, %0\n\t"
+                    "paddw	%2, %2, %3\n\t"
+                    "punpcklbh	%3, %0, %7\n\t"
+                    "paddw	%1, %1, %4\n\t"
+                    "punpckhbh	%4, %0, %7\n\t"
+                    "pmaddhw	%3, %3, %8\n\t"
+                    "pmaddhw	%4, %4, %9\n\t"
+                    "paddw	%2, %2, %3\n\t"
+                    "paddw	%2, %2, %4\n\t"
+                    : /* %0  */ "=&f" (in8),
+                      /* %1  */ "=f" (vs1),
+                      /* %2  */ "=f" (vs2),
+                      /* %3  */ "=&f" (in_lo),
+                      /* %4  */ "=&f" (in_hi)
+                    : /* %5  */ "f" (f * CHAR_BIT),
+                      /* %6  */ "f" (vk),
+                      /* %7  */ "f" (v0),
+                      /* %8  */ "f" (vord_lo),
+                      /* %9  */ "f" (vord_hi),
+                      /* %10 */ "f" (0x4e),
+                      /* %11 */ "m" (*buf),
+                      /* %12 */ "1" (vs1),
+                      /* %13 */ "2" (vs2)
+            );
+
+            buf += k;
+            k -= k;
+        }
+
+        /* add horizontal */
+        vs1 = paddw_u(vs1, (uint32x2_t)pshufh_u((uint16x4_t)v0, (uint16x4_t)vs1, 0x4E));
+        vs2 = paddw_u(vs2, (uint32x2_t)pshufh_u((uint16x4_t)v0, (uint16x4_t)vs2, 0x4E));
+        /* shake and roll */
+        s1 = (unsigned int)(unsigned long long)vs1;
+        s2 = (unsigned int)(unsigned long long)vs2;
+        /* modulo again in scalar code */
+    }
+
+    /* handle a possible trailer */
+    if (unlikely(len)) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--len);
+    MOD28(s1);
+    MOD28(s2);
+
+    /* return recombined sums */
+    return (s2 << 16) | s1;
+}
+
+#  elif defined(__mips_dsp)
+#    define HAVE_ADLER32_VEC
+#    define MIN_WORK 16
+#    define VNMAX ((5*NMAX)/2)
+
+typedef signed char v4i8 __attribute__((vector_size(4)));
+#    define SOV4 (sizeof(v4i8))
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1, s2;
+
+    /* split Adler-32 into component sums */
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    if (likely(len >= 2*SOV4)) {
+        static const v4i8 v1 = {1,1,1,1};
+        v4i8 vord_lo, vord_hi, vord;
+        /*
+         * yes, our sums should stay within 32 bit, that accs are
+         * 64 bit is to no (real) use to us.
+         */
+        unsigned int vs1, vs2, vs1h, vs2h;
+        v4i8 in4;
+        unsigned f, n;
+        unsigned k;
+        unsigned int srl;
+
+        if(host_is_bigendian()) {
+            vord_lo = (v4i8){8,7,6,5};
+            vord_hi = (v4i8){4,3,2,1};
+        } else {
+            vord_lo = (v4i8){5,6,7,8};
+            vord_hi = (v4i8){1,2,3,4};
+        }
+        vord = vord_hi;
+        /*
+         * Add stuff to achieve alignment
+         */
+        /* align hard down */
+        f = (unsigned) ALIGN_DOWN_DIFF(buf, SOV4);
+        n = SOV4 - f;
+        buf = (const unsigned char *)ALIGN_DOWN(buf, SOV4);
+
+        /* add n times s1 to s2 for start round */
+        s2 += s1 * n;
+
+        k = len < VNMAX ? len : VNMAX;
+        len -= k;
+
+        /* insert scalar start somewhere */
+        vs1 = s1;
+        vs2 = s2;
+        vs1h = 0;
+        vs2h = 0;
+
+        /* get input data */
+        srl = *(const unsigned int *)buf;
+        if(host_is_bigendian()) {
+            srl <<= f * CHAR_BIT;
+            srl >>= f * CHAR_BIT;
+        } else {
+            srl >>= f * CHAR_BIT;
+            srl <<= f * CHAR_BIT;
+        }
+        in4 = (v4i8)srl;
+
+        /* add all byte horizontal and add to old dword */
+        vs1 = __builtin_mips_dpau_h_qbl(vs1, in4, v1);
+        vs1 = __builtin_mips_dpau_h_qbr(vs1, in4, v1);
+
+        /* apply order, add 4 byte horizontal and add to old dword */
+        vs2 = __builtin_mips_dpau_h_qbl(vs2, in4, vord);
+        vs2 = __builtin_mips_dpau_h_qbr(vs2, in4, vord);
+
+        buf += SOV4;
+        k -= n;
+
+        if (likely(k >= 2*SOV4)) do {
+            unsigned int vs1_r, vs1h_r, t1, t2;
+            v4i8 in4h;
+
+            /*
+             * gcc and special regs...
+             * Since the mips acc are special, gcc fears to treat.
+             * Esp. gcc thinks it's cool to have the result ready in
+             * the gpr at the end of this main loop, so he hoist a
+             * move out of the acc and back in into the acc into the
+             * loop. m(
+             *
+             * So do it by hand, unrolled two times...
+             */
+            __asm__ (
+                    ".set noreorder\n\t"
+                    "mflo	%10, %q0\n\t"
+                    "xor	%6, %6, %6\n\t"
+                    "xor	%7, %7, %7\n"
+                    "1:\n\t"
+                    "lw	%4,(%9)\n\t"
+                    "lw	%5,4(%9)\n\t"
+                    "addu	%6, %6, %10\n\t"
+                    "dpau.h.qbl	%q0, %4, %14\n\t"
+                    "addiu	%8, %8, -8\n\t"
+                    "mflo	%10, %q1\n\t"
+                    "dpau.h.qbr	%q0, %4, %14\n\t"
+                    "dpau.h.qbl	%q1, %5, %14\n\t"
+                    "sltu	%11, %8, 8\n\t"
+                    "dpau.h.qbl	%q3, %4, %13\n\t"
+                    "addu	%7, %7 ,%10\n\t"
+                    "dpau.h.qbl	%q2, %5, %12\n\t"
+                    SZPRFX"addiu	%9, %9, 8\n\t"
+                    "mflo	%10, %q0\n\t"
+                    "dpau.h.qbr	%q1, %5, %14\n\t"
+                    "dpau.h.qbr	%q3, %4, %13\n\t"
+                    "beqz	%11, 1b\n\t"
+                    "dpau.h.qbr	%q2, %5, %12\n\t"
+                    ".set reorder"
+                    : /* %0  */ "=a" (vs1),
+                      /* %1  */ "=A" (vs1h),
+                      /* %2  */ "=A" (vs2),
+                      /* %3  */ "=A" (vs2h),
+                      /* %4  */ "=&r" (in4),
+                      /* %5  */ "=&r" (in4h),
+                      /* %6  */ "=&r" (vs1_r),
+                      /* %7  */ "=&r" (vs1h_r),
+                      /* %8  */ "=r" (k),
+                      /* %9  */ "=d" (buf),
+                      /* %10 */ "=r" (t1),
+                      /* %11 */ "=r" (t2)
+                    : /* %12 */ "r" (vord_lo),
+                      /* %13 */ "r" (vord_hi),
+                      /* %14 */ "r" (v1),
+                      /* %  */ "0" (vs1),
+                      /* %  */ "1" (vs1h),
+                      /* %  */ "2" (vs2),
+                      /* %  */ "3" (vs2h),
+                      /* %  */ "8" (k),
+                      /* %  */ "9" (buf)
+            );
+
+            /* chop vs1 round sum before multiplying by 8 */
+            CHOP(vs1_r);
+            CHOP(vs1h_r);
+            /* add all vs1 for 8 times */
+            vs2  += vs1_r  * 8;
+            vs2h += vs1h_r * 8;
+            /* chop the vectors to something in the range of BASE */
+            CHOP(vs2);
+            CHOP(vs2h);
+            CHOP(vs1);
+            CHOP(vs1h);
+            len += k;
+            k = len < VNMAX ? len : VNMAX;
+            len -= k;
+        } while (likely(k >= 2*SOV4));
+        vs1 += vs1h;
+        vs2 += vs2h;
+        /* a complete piece left? */
+        if (likely(k >= SOV4)) {
+            /* get input data */
+            in4 = *(const v4i8 *)buf;
+            vs2 += vs1 * 4;
+
+            /* add all byte horizontal and add to old dword */
+            vs1 = __builtin_mips_dpau_h_qbl(vs1, in4, v1);
+            vs1 = __builtin_mips_dpau_h_qbr(vs1, in4, v1);
+
+            /* apply order, add 4 byte horizontal and add to old dword */
+            vs2 = __builtin_mips_dpau_h_qbl(vs2, in4, vord);
+            vs2 = __builtin_mips_dpau_h_qbr(vs2, in4, vord);
+
+            k -= SOV4;
+            buf += SOV4;
+        }
+
+        if (likely(k)) {
+            unsigned int vk;
+
+            /*
+             * handle trailer
+             */
+            f = SOV4 - k;
+
+            /* add k times vs1 for this trailer */
+            vk = vs1 * k;
+
+            /* get input data */
+            srl = *(const unsigned int *)buf;
+            if(host_is_bigendian()) {
+                srl >>= f * CHAR_BIT;
+                srl <<= f * CHAR_BIT;
+            } else {
+                srl <<= f * CHAR_BIT;
+                srl >>= f * CHAR_BIT;
+            }
+            in4 = (v4i8)srl;
+
+            /* add all byte horizontal and add to old dword */
+            vs1 = __builtin_mips_dpau_h_qbl(vs1, in4, v1);
+            vs1 = __builtin_mips_dpau_h_qbr(vs1, in4, v1);
+
+            /* apply order, add 4 byte horizontal and add to old dword */
+            vs2 = __builtin_mips_dpau_h_qbl(vs2, in4, vord);
+            vs2 = __builtin_mips_dpau_h_qbr(vs2, in4, vord);
+
+            vs2 += vk;
+
+            buf += k;
+            k -= k;
+        }
+
+        /* shake and roll */
+        s1 = (unsigned int)vs1;
+        s2 = (unsigned int)vs2;
+        /* modulo again in scalar code */
+    }
+
+    /* handle a possible trailer */
+    if (unlikely(len)) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--len);
+    /* s2 should be small here */
+    MOD28(s1);
+    MOD28(s2);
+
+    /* return recombined sums */
+    return (s2 << 16) | s1;
+}
+#  endif
+#endif
diff --git a/ppc/adler32.c b/ppc/adler32.c
new file mode 100644
index 0000000..ce7f3f8
--- /dev/null
+++ b/ppc/adler32.c
@@ -0,0 +1,261 @@
+/*
+ * adler32.c -- compute the Adler-32 checksum of a data stream
+ *   ppc implementation
+ * Copyright (C) 1995-2007 Mark Adler
+ * Copyright (C) 2009-2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* @(#) $Id$ */
+
+/*
+ * We use the Altivec PIM vector stuff, but still, this is only
+ * tested with GCC, and prop. uses some GCC specifics (like GCC
+ * understands vector types and you can simply write a += b)
+ */
+#if defined(__ALTIVEC__) && defined(__GNUC__)
+# define HAVE_ADLER32_VEC
+/* it needs some bytes till the vec version gets up to speed... */
+# define MIN_WORK 64
+# include <altivec.h>
+
+/*
+ * Depending on length, this can be slower (short length < 64 bytes),
+ * much faster (our beloved 128kb 22.2s generic to 3.4s vec, but cache
+ * is important...), to a little faster (very long length, 1.6MB, 47.6s
+ * to 36s), which is prop. only capped by memory bandwith.
+ * (The orig. 128k case was slower in AltiVec, because AltiVec loads
+ * are always uncached and trigger no HW prefetching, because that is
+ * what you often need with mass data manipulation (not poisen your
+ * cache, movntq), instead you have to do it for yourself (data stream
+ * touch). With 128k it could be cleanly seen: no prefetch, half as slow
+ * as generic, but comment out the memory load -> 3s. With proper prefetch
+ * we are at 3.4s. So AltiVec can execute these "expensive" FMA quite
+ * fast (even without fancy unrolling), only the data does not arrive
+ * fast enough. In cases where the working set does not fit into cache
+ * it simply cannot be delivered fast enough over the FSB/Mem).
+ * Still we have to prefetch, or we are slow as hell.
+ */
+
+# define SOVUC (sizeof(vector unsigned char))
+
+/* can be propably more, since we do not have the x86 psadbw 64 bit sum */
+# define VNMAX (6*NMAX)
+
+/* ========================================================================= */
+local inline vector unsigned char vec_identl(level)
+    unsigned int level;
+{
+    return vec_lvsl(level, (const unsigned char *)0);
+}
+
+/* ========================================================================= */
+local inline vector unsigned char vec_ident_rev(void)
+{
+    return vec_xor(vec_identl(0), vec_splat_u8(15));
+}
+
+/* ========================================================================= */
+/* multiply two 32 bit ints, return the low 32 bit */
+local inline vector unsigned int vec_mullw(vector unsigned int a, vector unsigned int b)
+{
+    vector unsigned int v16   = vec_splat_u32(-16);
+    vector unsigned int v0_32 = vec_splat_u32(0);
+    vector unsigned int swap, low, high;
+
+    swap = vec_rl(b, v16);
+    low  = vec_mulo((vector unsigned short)a, (vector unsigned short)b);
+    high = vec_msum((vector unsigned short)a, (vector unsigned short)swap, v0_32);
+    high = vec_sl(high, v16);
+    return vec_add(low, high);
+}
+
+/* ========================================================================= */
+local inline vector unsigned int vector_chop(vector unsigned int x)
+{
+    vector unsigned int y;
+    vector unsigned int vsh;
+
+    vsh = vec_splat_u32(1);
+    vsh = vec_sl(vsh, vec_splat_u32(4));
+
+    y = vec_sl(x, vsh);
+    y = vec_sr(y, vsh);
+    x = vec_sr(x, vsh);
+    y = vec_sub(y, x);
+    x = vec_sl(x, vec_splat_u32(4));
+    x = vec_add(x, y);
+    return x;
+}
+
+/* ========================================================================= */
+local inline vector unsigned int vector_load_one_u32(unsigned int x)
+{
+    vector unsigned int val = vec_lde(0, &x);
+    vector unsigned char vperm, mask;
+
+    mask = (vector unsigned char)vec_cmpgt(vec_identl(0), vec_splat_u8(3));
+    vperm = (vector unsigned char)vec_splat((vector unsigned int)vec_lvsl(0, &x), 0);
+    val = vec_perm(val, val, vperm);
+    val = vec_sel(val, vec_splat_u32(0), (vector unsigned int)mask);
+    return val;
+}
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1, s2;
+
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    if (likely(len >= 2*SOVUC)) {
+        vector unsigned int   vsh = vec_splat_u32(4);
+        vector unsigned char   v1 = vec_splat_u8(1);
+        vector unsigned char vord;
+        vector unsigned char   v0 = vec_splat_u8(0);
+        vector unsigned int vs1, vs2;
+        vector unsigned char in16, vord_a, v1_a, vperm;
+        unsigned int f, n;
+        unsigned int k, block_num;
+
+        /*
+         * if i understand the Altivec PEM right, little
+         * endian impl. should have the data reversed on
+         * load, so the big endian vorder works.
+         */
+        vord = vec_ident_rev() + v1;
+        block_num = DIV_ROUNDUP(len, 512); /* 32 block size * 16 bytes */
+        f  = 512;
+        f |= block_num >= 256 ? 0 : block_num << 16;
+        vec_dst(buf, f, 2);
+        /*
+         * Add stuff to achieve alignment
+         */
+        /* swizzle masks in place */
+        vperm  = vec_lvsl(0, buf);
+        vord_a = vec_perm(vord, v0, vperm);
+        v1_a   = vec_perm(v1, v0, vperm);
+        vperm  = vec_lvsr(0, buf);
+        vord_a = vec_perm(v0, vord_a, vperm);
+        v1_a   = vec_perm(v0, v1_a, vperm);
+
+        /* align hard down */
+        f = (unsigned) ALIGN_DOWN_DIFF(buf, SOVUC);
+        n = SOVUC - f;
+        buf = (const unsigned char *)ALIGN_DOWN(buf, SOVUC);
+
+        /* add n times s1 to s2 for start round */
+        s2 += s1 * n;
+
+        k = len < VNMAX ? (unsigned)len : VNMAX;
+        len -= k;
+
+        /* insert scalar start somewhere */
+        vs1 = vector_load_one_u32(s1);
+        vs2 = vector_load_one_u32(s2);
+
+        /* get input data */
+        in16 = vec_ldl(0, buf);
+
+        /* mask out excess data, add 4 byte horizontal and add to old dword */
+        vs1 = vec_msum(in16, v1_a, vs1);
+
+        /* apply order, masking out excess data, add 4 byte horizontal and add to old dword */
+        vs2 = vec_msum(in16, vord_a, vs2);
+
+        buf += SOVUC;
+        k -= n;
+
+        if (likely(k >= SOVUC)) do {
+            vector unsigned int vs1_r = vec_splat_u32(0);
+            f  = 512;
+            f |= block_num >= 256 ? 0 : block_num << 16;
+            vec_dst(buf, f, 2);
+
+            do {
+                /* get input data */
+                in16 = vec_ldl(0, buf);
+
+                /* add vs1 for this round */
+                vs1_r += vs1;
+
+                /* add 4 byte horizontal and add to old dword */
+                vs1 = vec_sum4s(in16, vs1);
+                /* apply order, add 4 byte horizontal and add to old dword */
+                vs2 = vec_msum(in16, vord, vs2);
+
+                buf += SOVUC;
+                k -= SOVUC;
+            } while (k >= SOVUC);
+            /* chop vs1 round sum before multiplying by 16 */
+            vs1_r = vector_chop(vs1_r);
+            /* add all vs1 for 16 times */
+            vs2 += vec_sl(vs1_r, vsh);
+            /* chop the vectors to something in the range of BASE */
+            vs2 = vector_chop(vs2);
+            vs1 = vector_chop(vs1);
+            len += k;
+            k = len < VNMAX ? (unsigned)len : VNMAX;
+            block_num = DIV_ROUNDUP(len, 512); /* 32 block size * 16 bytes */
+            len -= k;
+        } while (likely(k >= SOVUC));
+
+        if (likely(k)) {
+            vector unsigned int vk;
+            /*
+             * handle trailer
+             */
+            f = SOVUC - k;
+            /* swizzle masks in place */
+            vperm  = vec_identl(f);
+            vord_a = vec_perm(vord, v0, vperm);
+            v1_a   = vec_perm(v1, v0, vperm);
+
+            /* add k times vs1 for this trailer */
+            vk = (vector unsigned int)vec_lvsl(0, (unsigned *)(intptr_t)k);
+            vk = (vector unsigned)vec_mergeh(v0, (vector unsigned char)vk);
+            vk = (vector unsigned)vec_mergeh((vector unsigned short)v0, (vector unsigned short)vk);
+            vk = vec_splat(vk, 0);
+            vs2 += vec_mullw(vs1, vk);
+
+            /* get input data */
+            in16 = vec_ldl(0, buf);
+
+            /* mask out excess data, add 4 byte horizontal and add to old dword */
+            vs1 = vec_msum(in16, v1_a, vs1);
+            /* apply order, masking out excess data, add 4 byte horizontal and add to old dword */
+            vs2 = vec_msum(in16, vord_a, vs2);
+
+            buf += k;
+            k -= k;
+        }
+
+        vec_dss(2);
+
+        /* add horizontal */
+        /* stuff should be choped so no proplem with signed saturate */
+        vs1 = (vector unsigned int)vec_sums((vector int)vs1, vec_splat_s32(0));
+        vs2 = (vector unsigned int)vec_sums((vector int)vs2, vec_splat_s32(0));
+        /* shake and roll */
+        vs1 = vec_splat(vs1, 3);
+        vs2 = vec_splat(vs2, 3);
+        vec_ste(vs1, 0, &s1);
+        vec_ste(vs2, 0, &s2);
+        /* after horizontal add, modulo again in scalar code */
+    }
+
+    if (unlikely(len)) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--len);
+    MOD28(s1);
+    MOD28(s2);
+
+    return (s2 << 16) | s1;
+}
+
+#endif
diff --git a/sparc/adler32.c b/sparc/adler32.c
new file mode 100644
index 0000000..4e9ffb2
--- /dev/null
+++ b/sparc/adler32.c
@@ -0,0 +1,354 @@
+/*
+ * adler32.c -- compute the Adler-32 checksum of a data stream
+ *   sparc implementation
+ * Copyright (C) 1995-2007 Mark Adler
+ * Copyright (C) 2009-2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* @(#) $Id$ */
+
+/* we use inline asm, so GCC it is */
+#if defined(HAVE_VIS) && defined(__GNUC__)
+#  define HAVE_ADLER32_VEC
+#  define MIN_WORK 512
+#  define VNMAX ((5*NMAX)/2)
+#  define SOVV (sizeof(unsigned long long))
+
+/*
+ * SPARC CPUs gained a SIMD extention with the UltraSPARC I, called
+ * VIS, implemented in their FPU. We can build adler32 with it.
+ *
+ * But Note:
+ * - Do not use it with Niagara or other CPUs like it. They have a
+ *   shared FPU (T1: 1 FPU for all up to 8 cores, T2: 1 FPU for 8 threads)
+ *   and to make matters worse the code does not seem to work there
+ *   (binary which creates correct results on other SPARC creates wrong
+ *   result on T1)
+ * - There is no clear preprocesor define which tells us if we have VIS.
+ *   Often the tool chain even disguises a sparcv9 as a sparcv8
+ *   (pre-UltraSPARC) to not confuse the userspace.
+ * - The code has a high startup cost
+ * - The code only handles big endian
+ *
+ * For these reasons this code is not automatically enabled and you have
+ * to define HAVE_VIS as a switch to enable it. We can not easily provide a
+ * dynamic runtime switch. The CPU has make and model encoded in the Processor
+ * Status Word (PSW), but reading the PSW is a privilidged instruction (same
+ * as PowerPC...).
+ */
+
+/* ========================================================================= */
+local inline unsigned long long fzero(void)
+{
+    unsigned long long t;
+    asm ("fzero %0" : "=e" (t));
+    return t;
+}
+
+/* ========================================================================= */
+local inline unsigned long long fpmerge_lo(unsigned long long a, unsigned long long b)
+{
+    unsigned long long t;
+    asm ("fpmerge	%L1, %L2, %0" : "=e" (t) : "f" (a), "f" (b));
+    return t;
+}
+
+/* ========================================================================= */
+local inline unsigned long long fpmerge_hi(unsigned long long a, unsigned long long b)
+{
+    unsigned long long t;
+    asm ("fpmerge	%H1, %H2, %0" : "=e" (t) : "f" (a), "f" (b));
+    return t;
+}
+#define pextlbh(x) fpmerge_lo(fzero(), x)
+#define pexthbh(x) fpmerge_hi(fzero(), x)
+
+/* ========================================================================= */
+local inline unsigned long long fpadd32(unsigned long long a, unsigned long long b)
+{
+    unsigned long long t;
+    asm ("fpadd32	%1, %2, %0" : "=e" (t) : "%e" (a), "e" (b));
+    return t;
+}
+
+/* ========================================================================= */
+local inline unsigned long long fpadd16(unsigned long long a, unsigned long long b)
+{
+    unsigned long long t;
+    asm ("fpadd16	%1, %2, %0" : "=e" (t) : "%e" (a), "e" (b));
+    return t;
+}
+
+/* ========================================================================= */
+local inline unsigned long long fpsub32(unsigned long long a, unsigned long long b)
+{
+    unsigned long long t;
+    asm ("fpsub32	%1, %2, %0" : "=e" (t) : "e" (a), "e" (b));
+    return t;
+}
+
+/* ========================================================================= */
+local inline unsigned long long pdist(unsigned long long a, unsigned long long b, unsigned long long acc)
+{
+    asm ("pdist	%1, %2, %0" : "=e" (acc) : "e" (a), "e" (b), "0" (acc));
+    return acc;
+}
+
+/* ========================================================================= */
+local inline unsigned long long fmuld8ulx16_lo(unsigned long long a, unsigned long long b)
+{
+    unsigned long long t;
+    asm ("fmuld8ulx16	%L1, %L2, %0" : "=e" (t) : "f" (a), "f" (b));
+    return t;
+}
+
+/* ========================================================================= */
+local inline unsigned long long fmuld8ulx16_hi(unsigned long long a, unsigned long long b)
+{
+    unsigned long long t;
+    asm ("fmuld8ulx16	%H1, %H2, %0" : "=e" (t) : "f" (a), "f" (b));
+    return t;
+}
+
+/* ========================================================================= */
+local inline unsigned long long fmuld8sux16_lo(unsigned long long a, unsigned long long b)
+{
+    unsigned long long t;
+    asm ("fmuld8sux16	%L1, %L2, %0" : "=e" (t) : "f" (a), "f" (b));
+    return t;
+}
+
+/* ========================================================================= */
+local inline unsigned long long fmuld8sux16_hi(unsigned long long a, unsigned long long b)
+{
+    unsigned long long t;
+    asm ("fmuld8sux16	%H1, %H2, %0" : "=e" (t) : "f" (a), "f" (b));
+    return t;
+}
+
+/* ========================================================================= */
+local inline unsigned long long fpmul16x16x32_lo(unsigned long long a, unsigned long long b)
+{
+    unsigned long long r_l = fmuld8ulx16_lo(a, b);
+    unsigned long long r_h = fmuld8sux16_lo(a, b);
+    return fpadd32(r_l, r_h);
+}
+
+/* ========================================================================= */
+local inline unsigned long long fpmul16x16x32_hi(unsigned long long a, unsigned long long b)
+{
+    unsigned long long r_l = fmuld8ulx16_hi(a, b);
+    unsigned long long r_h = fmuld8sux16_hi(a, b);
+    return fpadd32(r_l, r_h);
+}
+
+/* ========================================================================= */
+local inline unsigned long long fpand(unsigned long long a, unsigned long long b)
+{
+    unsigned long long t;
+    asm ("fand	%1, %2, %0" : "=e" (t) : "e" (a), "e" (b));
+    return t;
+}
+
+/* ========================================================================= */
+local inline unsigned long long fpandnot2(unsigned long long a, unsigned long long b)
+{
+    unsigned long long t;
+    asm ("fandnot2	%1, %2, %0" : "=e" (t) : "e" (a), "e" (b));
+    return t;
+}
+
+/* ========================================================================= */
+local inline void *alignaddr(const void *ptr, int off)
+{
+    void *t;
+    asm volatile ("alignaddr	%1, %2, %0" : "=r" (t) : "r" (ptr), "r" (off));
+    return t;
+}
+
+/* ========================================================================= */
+local inline void *alignaddrl(const void *ptr, int off)
+{
+    void *t;
+    asm volatile ("alignaddrl	%1, %2, %0" : "=r" (t) : "r" (ptr), "r" (off));
+    return t;
+}
+
+/* ========================================================================= */
+local inline unsigned long long faligndata(unsigned long long a, unsigned long long b)
+{
+    unsigned long long ret;
+    asm volatile ("faligndata	%1, %2, %0" : "=e" (ret) : "e" (a), "e" (b));
+    return ret;
+}
+
+/* ========================================================================= */
+local inline unsigned long long vector_chop(unsigned long long x)
+{
+    unsigned long long mask = 0x0000ffff0000ffffull;
+    unsigned long long y = fpand(x, mask);
+    x = fpandnot2(x, mask);
+    x >>= 16;
+    y = fpsub32(y, x);
+    x = fpadd32(x, x); /* << 1 */
+    x = fpadd32(x, x); /* << 1 */
+    x = fpadd32(x, x); /* << 1 */
+    x = fpadd32(x, x); /* << 1 */
+    return fpadd32(x, y);
+}
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1, s2;
+
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    if (likely(len >= 2 * SOVV)) {
+        unsigned long long vorder_lo = 0x0004000300020001ULL;
+        unsigned long long vorder_hi = 0x0008000700060005ULL;
+        unsigned long long vs1, vs2;
+        unsigned long long v0 = fzero();
+        unsigned long long in;
+        const unsigned char *o_buf;
+        unsigned int k, f, n;
+
+        /* align hard down */
+        f = (unsigned int)ALIGN_DOWN_DIFF(buf, SOVV);
+        n = SOVV - f;
+
+        /* add n times s1 to s2 for start round */
+        s2 += s1 * n;
+
+        /* insert scalar start somehwere */
+        vs1 = s1;
+        vs2 = s2;
+
+        k = len < VNMAX ? len : VNMAX;
+        len -= k;
+
+        /* get input data */
+        o_buf = buf;
+        buf = alignaddr(buf, 0);
+        in = *(const unsigned long long *)buf;
+        {
+            unsigned long long vs2t;
+            /* swizzle data in place */
+            in = faligndata(in, v0);
+            if(f) {
+                alignaddrl(o_buf, 0);
+                in = faligndata(v0, in);
+            }
+            /* add horizontal and acc */
+            vs1 = pdist(in, v0, vs1);
+            /* extract, apply order and acc */
+            vs2t = pextlbh(in);
+            vs2 = fpadd32(vs2, fpmul16x16x32_lo(vs2t, vorder_lo));
+            vs2 = fpadd32(vs2, fpmul16x16x32_hi(vs2t, vorder_lo));
+            vs2t = pexthbh(in);
+            vs2 = fpadd32(vs2, fpmul16x16x32_lo(vs2t, vorder_hi));
+            vs2 = fpadd32(vs2, fpmul16x16x32_hi(vs2t, vorder_hi));
+        }
+        buf += SOVV;
+        k   -= n;
+
+        if (likely(k >= SOVV)) do {
+            unsigned long long vs1_r = fzero();
+            do {
+                unsigned long long vs2l = fzero(), vs2h = fzero();
+                unsigned j;
+
+                j = (k/SOVV) > 127 ? 127 : k/SOVV;
+                k -= j * SOVV;
+                do {
+                    /* get input data */
+                    in = *(const unsigned long long *)buf;
+                    buf += SOVV;
+                    /* add vs1 for this round */
+                    vs1_r = fpadd32(vs1_r, vs1);
+                    /* add horizontal and acc */
+                    vs1 = pdist(in, v0, vs1);
+                    /* extract */
+                    vs2l = fpadd16(vs2l, pextlbh(in));
+                    vs2h = fpadd16(vs2h, pexthbh(in));
+                } while (--j);
+                vs2 = fpadd32(vs2, fpmul16x16x32_lo(vs2l, vorder_lo));
+                vs2 = fpadd32(vs2, fpmul16x16x32_hi(vs2l, vorder_lo));
+                vs2 = fpadd32(vs2, fpmul16x16x32_lo(vs2h, vorder_hi));
+                vs2 = fpadd32(vs2, fpmul16x16x32_hi(vs2h, vorder_hi));
+            } while (k >= SOVV);
+            /* chop vs1 round sum before multiplying by 8 */
+            vs1_r = vector_chop(vs1_r);
+            /* add vs1 for this round (8 times) */
+            vs1_r = fpadd32(vs1_r, vs1_r); /* *2 */
+            vs1_r = fpadd32(vs1_r, vs1_r); /* *4 */
+            vs1_r = fpadd32(vs1_r, vs1_r); /* *8 */
+            vs2   = fpadd32(vs2, vs1_r);
+            /* chop both sums */
+            vs2 = vector_chop(vs2);
+            vs1 = vector_chop(vs1);
+            len += k;
+            k = len < VNMAX ? len : VNMAX;
+            len -= k;
+        } while (likely(k >= SOVV));
+
+        if (likely(k)) {
+            /* handle trailer */
+            unsigned long long t, r, vs2t;
+            unsigned int k_m;
+
+            /* get input data */
+            in = *(const unsigned long long *)buf;
+
+            /* swizzle data in place */
+            alignaddr(buf, k);
+            in = faligndata(v0, in);
+
+            /* add k times vs1 for this trailer */
+            /* since VIS muls are painfull, use the russian peasant method, k is small */
+            t = 0;
+            r = vs1;
+            k_m = k;
+            do {
+                if(k_m & 1)
+                    t = fpadd32(t, r); /* add to result if odd */
+                r = fpadd32(r, r); /* *2 */
+                k_m >>= 1; /* /2 */
+            } while (k_m);
+            vs2 = fpadd32(vs2, t);
+
+            /* add horizontal and acc */
+            vs1 = pdist(in, v0, vs1);
+            /* extract, apply order and acc */
+            vs2t = pextlbh(in);
+            vs2  = fpadd32(vs2, fpmul16x16x32_lo(vs2t, vorder_lo));
+            vs2  = fpadd32(vs2, fpmul16x16x32_hi(vs2t, vorder_lo));
+            vs2t = pexthbh(in);
+            vs2  = fpadd32(vs2, fpmul16x16x32_lo(vs2t, vorder_hi));
+            vs2  = fpadd32(vs2, fpmul16x16x32_hi(vs2t, vorder_hi));
+
+            buf += k;
+            k   -= k;
+        }
+        /* vs1 is one giant 64 bit sum, but we only calc to 32 bit */
+        s1 = vs1;
+        /* add both vs2 sums */
+        s2 = (vs2 & 0xFFFFFFFFUL) + (vs2 >> 32);
+    }
+
+    if (unlikely(len)) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--len);
+    /* at this point we should not have so big s1 & s2 */
+    MOD28(s1);
+    MOD28(s2);
+    /* return recombined sums */
+    return (s2 << 16) | s1;
+}
+#endif
diff --git a/tile/adler32.c b/tile/adler32.c
new file mode 100644
index 0000000..c65347b
--- /dev/null
+++ b/tile/adler32.c
@@ -0,0 +1,255 @@
+/*
+ * adler32.c -- compute the Adler-32 checksum of a data stream
+ *   Tile implementation
+ * Copyright (C) 1995-2007 Mark Adler
+ * Copyright (C) 2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* @(#) $Id$ */
+
+#if defined(__GNUC__)
+#  if defined(__tilegx__)
+#    define HAVE_ADLER32_VEC
+#    define MIN_WORK 32
+// TODO: VNMAX could prop. be a little higher?
+#    define VNMAX (2*NMAX+((9*NMAX)/10))
+
+#    define SOUL (sizeof(unsigned long))
+
+/* ========================================================================= */
+local inline unsigned long v1ddotpua(unsigned long d, unsigned long a, unsigned long b)
+{
+    return __insn_v1ddotpua(d, a, b);
+}
+
+/* ========================================================================= */
+local inline unsigned long v4shl(unsigned long a, unsigned long b)
+{
+    return __insn_v4shl(a, b);
+}
+
+/* ========================================================================= */
+local inline unsigned long v4shru(unsigned long a, unsigned long b)
+{
+    return __insn_v4shru(a, b);
+}
+
+/* ========================================================================= */
+local inline unsigned long v4sub(unsigned long a, unsigned long b)
+{
+    return __insn_v4sub(a, b);
+}
+
+/* ========================================================================= */
+local inline unsigned long v4add(unsigned long a, unsigned long b)
+{
+    return __insn_v4add(a, b);
+}
+
+/* ========================================================================= */
+local inline unsigned long vector_chop(unsigned long x)
+{
+    unsigned long y;
+
+    y = v4shl(x, 16);
+    y = v4shru(y, 16);
+    x = v4shru(x, 16);
+    y = v4sub(y, x);
+    x = v4shl(x, 4);
+    x = v4add(x, y);
+    return x;
+}
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1, s2;
+    unsigned k;
+
+    /* split Adler-32 into component sums */
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    /* align input */
+    k    = ALIGN_DIFF(buf, SOUL);
+    len -= k;
+    if (k) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--k);
+
+    k = len < VNMAX ? len : VNMAX;
+    len -= k;
+    if (likely(k >= 2 * SOUL)) {
+        const unsigned long v1 = 0x0101010101010101ul;
+        unsigned long vs1 = s1, vs2 = s2;
+        unsigned long vorder;
+
+        if(host_is_bigendian())
+            vorder = 0x0807060504030201;
+        else
+            vorder = 0x0102030405060708;
+
+        do {
+            unsigned long vs1_r = 0;
+            do {
+                /* get input data */
+                unsigned long in = *(const unsigned long *)buf;
+                /* add vs1 for this round */
+                vs1_r += vs1; /* we don't overflow, so normal add */
+                /* add horizontal & acc vs1 */
+                vs1 = v1ddotpua(vs1, in, v1); /* only into x0 pipeline */
+                /* mul, add & acc vs2 */
+                vs2 = v1ddotpua(vs2, in, vorder); /* only in x0 pipe */
+                buf += SOUL;
+                k -= SOUL;
+            } while (k >= SOUL);
+            /* chop vs1 round sum before multiplying by 8 */
+            vs1_r = vector_chop(vs1_r);
+            /* add vs1 for this round (8 times) */
+            vs1_r = v4shl(vs1_r, 3);
+            vs2 += vs1_r;
+            /* chop both sums */
+            vs2 = vector_chop(vs2);
+            vs1 = vector_chop(vs1);
+            len += k;
+            k = len < VNMAX ? len : VNMAX;
+            len -= k;
+        } while (likely(k >= SOUL));
+        s1 = (vs1 & 0xffffffff) + (vs1 >> 32);
+        s2 = (vs2 & 0xffffffff) + (vs2 >> 32);
+    }
+
+    /* handle trailer */
+    if (unlikely(k)) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--k);
+    /* at this point s1 & s2 should be in range */
+    MOD28(s1);
+    MOD28(s2);
+
+    /* return recombined sums */
+    return (s2 << 16) | s1;
+}
+#  else
+#    define HAVE_ADLER32_VEC
+#    define MIN_WORK 32
+// TODO: VNMAX could be higher
+#    define VNMAX NMAX
+
+#    define SOUL (sizeof(unsigned long))
+
+/* ========================================================================= */
+local inline unsigned long sadab_u(unsigned long d, unsigned long a, unsigned long b)
+{
+    return __insn_sadab_u(d, a, b);
+}
+
+/* ========================================================================= */
+local inline unsigned long inthb(unsigned long a, unsigned long b)
+{
+    return __insn_inthb(a, b);
+}
+
+/* ========================================================================= */
+local inline unsigned long intlb(unsigned long a, unsigned long b)
+{
+    return __insn_intlb(a, b);
+}
+
+/* ========================================================================= */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1, s2;
+    unsigned k;
+
+    /* split Adler-32 into component sums */
+    s1 = adler & 0xffff;
+    s2 = (adler >> 16) & 0xffff;
+
+    /* align input */
+    k    = ALIGN_DIFF(buf, SOUL);
+    len -= k;
+    if (k) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--k);
+
+    k = len < VNMAX ? len : VNMAX;
+    len -= k;
+    if (likely(k >= 2 * SOUL)) {
+        unsigned long vs1 = s1, vs2 = s2;
+
+        do {
+            unsigned long vs1_r = 0;
+            do {
+                unsigned long a, b, c, d;
+                unsigned long vs2l = 0, vs2h = 0;
+                unsigned j;
+
+                j = k > 257 * SOUL ? 257 : k/SOUL;
+                k -= j * SOUL;
+                do {
+                    /* get input data */
+                    unsigned long in = *(const unsigned long *)buf;
+                    /* add vs1 for this round */
+                    vs1_r += vs1;
+                    /* add horizontal */
+                    vs1 = sadab_u(vs1, in, 0);
+                    /* extract */
+                    vs2l += intlb(0, in);
+                    vs2h += inthb(0, in);
+                    buf += SOUL;
+                } while (--j);
+                /* split vs2 */
+                if(host_is_bigendian()) {
+                    a = (vs2h >> 16) & 0x0000ffff;
+                    b = (vs2h      ) & 0x0000ffff;
+                    c = (vs2l >> 16) & 0x0000ffff;
+                    d = (vs2l      ) & 0x0000ffff;
+                } else {
+                    a = (vs2l      ) & 0x0000ffff;
+                    b = (vs2l >> 16) & 0x0000ffff;
+                    c = (vs2h      ) & 0x0000ffff;
+                    d = (vs2h >> 16) & 0x0000ffff;
+                }
+                /* mull&add vs2 horiz. */
+                vs2 += 4*a + 3*b + 2*c + 1*d;
+            } while (k >= SOUL);
+            /* reduce vs1 round sum before multiplying by 4 */
+            CHOP(vs1_r);
+            /* add vs1 for this round (4 times) */
+            vs2 += vs1_r * 4;
+            /* reduce both sums */
+            CHOP(vs2);
+            CHOP(vs1);
+            len += k;
+            k = len < VNMAX ? len : VNMAX;
+            len -= k;
+        } while (likely(k >= SOUL));
+        s1 = vs1;
+        s2 = vs2;
+    }
+
+    /* handle trailer */
+    if (unlikely(k)) do {
+        s1 += *buf++;
+        s2 += s1;
+    } while (--k);
+    /* at this point we should not have so big s1 & s2 */
+    MOD28(s1);
+    MOD28(s2);
+
+    /* return recombined sums */
+    return (s2 << 16) | s1;
+}
+#  endif
+#endif
diff --git a/x86/adler32.c b/x86/adler32.c
new file mode 100644
index 0000000..e0fda88
--- /dev/null
+++ b/x86/adler32.c
@@ -0,0 +1,1220 @@
+/*
+ * adler32.c -- compute the Adler-32 checksum of a data stream
+ *   x86 implementation
+ * Copyright (C) 1995-2007 Mark Adler
+ * Copyright (C) 2009-2011 Jan Seiffert
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* @(#) $Id$ */
+
+#if GCC_VERSION_GE(207)
+#  define GCC_ATTR_CONSTRUCTOR __attribute__((__constructor__))
+#else
+#  define VEC_NO_GO
+#endif
+
+#if GCC_VERSION_GE(203)
+#  define GCC_ATTR_ALIGNED(x) __attribute__((__aligned__(x)))
+#else
+#  define VEC_NO_GO
+#endif
+
+/* inline asm, so only on GCC (or compatible) */
+#if defined(__GNUC__) && !defined(VEC_NO_GO)
+#  define HAVE_ADLER32_VEC
+#  define MIN_WORK 64
+
+#  ifdef __x86_64__
+#    define PICREG "%%rbx"
+#  else
+#    define PICREG "%%ebx"
+#  endif
+
+/* ========================================================================= */
+local const struct { short d[24]; } vord GCC_ATTR_ALIGNED(16) = {
+    {1,1,1,1,1,1,1,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1}
+};
+
+/* ========================================================================= */
+local const struct { char d[16]; } vord_b GCC_ATTR_ALIGNED(16) = {
+    {16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1}
+};
+
+/* ========================================================================= */
+local noinline const Bytef *adler32_jumped(buf, s1, s2, k)
+    const Bytef *buf;
+    unsigned int *s1;
+    unsigned int *s2;
+    unsigned int k;
+{
+    unsigned int t;
+    unsigned n = k % 16;
+    buf += n;
+    k = (k / 16) + 1;
+
+    __asm__ __volatile__ (
+#  ifdef __x86_64__
+#    define CLOB "&"
+            "lea	1f(%%rip), %q4\n\t"
+            "lea	(%q4,%q5,8), %q4\n\t"
+            "jmp	*%q4\n\t"
+#  else
+#    ifndef __PIC__
+#      define CLOB
+            "lea	1f(,%5,8), %4\n\t"
+#    else
+#      define CLOB
+            "lea	1f-3f(,%5,8), %4\n\t"
+            "call	9f\n"
+            "3:\n\t"
+#    endif
+            "jmp	*%4\n\t"
+#    ifdef __PIC__
+            ".p2align 1\n"
+            "9:\n\t"
+            "addl	(%%esp), %4\n\t"
+            "ret\n\t"
+#    endif
+#  endif
+            ".p2align 1\n"
+            "2:\n\t"
+#  ifdef __i386
+            ".byte 0x3e\n\t"
+#  endif
+            "add	$0x10, %2\n\t"
+            ".p2align 1\n"
+            "1:\n\t"
+            /* 128 */
+            "movzbl	-16(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /* 120 */
+            "movzbl	-15(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /* 112 */
+            "movzbl	-14(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /* 104 */
+            "movzbl	-13(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*  96 */
+            "movzbl	-12(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*  88 */
+            "movzbl	-11(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*  80 */
+            "movzbl	-10(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*  72 */
+            "movzbl	 -9(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*  64 */
+            "movzbl	 -8(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*  56 */
+            "movzbl	 -7(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*  48 */
+            "movzbl	 -6(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*  40 */
+            "movzbl	 -5(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*  32 */
+            "movzbl	 -4(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*  24 */
+            "movzbl	 -3(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*  16 */
+            "movzbl	 -2(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*   8 */
+            "movzbl	 -1(%2), %4\n\t"	/* 4 */
+            "add	%4, %0\n\t"	/* 2 */
+            "add	%0, %1\n\t"	/* 2 */
+            /*   0 */
+            "dec	%3\n\t"
+            "jnz	2b"
+        : /* %0 */ "=R" (*s1),
+          /* %1 */ "=R" (*s2),
+          /* %2 */ "=abdSD" (buf),
+          /* %3 */ "=c" (k),
+          /* %4 */ "="CLOB"R" (t)
+        : /* %5 */ "r" (16 - n),
+          /*    */ "0" (*s1),
+          /*    */ "1" (*s2),
+          /*    */ "2" (buf),
+          /*    */ "3" (k)
+        : "cc", "memory"
+    );
+
+    return buf;
+}
+
+
+
+#if 0 && (HAVE_BINUTILS-0) >= 222
+        /*
+         * 2013 Intel will hopefully bring the Haswell CPUs,
+         * which hopefully will have AVX2, which brings integer
+         * ops to the full width AVX regs.
+         */
+        "2:\n\t"
+        "mov	$256, %1\n\t"
+        "cmp	%1, %3\n\t"
+        "cmovb	%3, %1\n\t"
+        "and	$-32, %1\n\t"
+        "sub	%1, %3\n\t"
+        "shr	$5, %1\n\t"
+        "vpxor	%%xmm6, %%xmm6\n\t"
+        ".p2align 4,,7\n"
+        ".p2align 3\n"
+        "1:\n\t"
+        "vmovdqa	(%0), %%ymm0\n\t"
+        "prefetchnta	0x70(%0)\n\t"
+        "vpaddd	%%ymm3, %%ymm7, %%ymm7\n\t"
+        "add	$32, %0\n\t"
+        "dec	%1\n\t"
+        "vpsadbw	%%ymm4, %%ymm0, %%ymm1\n\t"
+        "vpmaddubsw	%%ymm5, %%ymm0, %%ymm0\n\t"
+        "vpaddd	%%ymm1, %%ymm3, %%ymm3\n\t"
+        "vpaddw	%%ymm0, %%ymm6, %%ymm6\n\t"
+        "jnz	1b\n\t"
+        "vpunpckhwd	%%ymm4, %%ymm6, %%xmm0\n\t"
+        "vpunpcklwd	%%ymm4, %%ymm6, %%ymm6\n\t"
+        "vpaddd	%%ymm0, %%ymm2, %%ymm2\n\t"
+        "vpaddd	%%ymm6, %%ymm2, %%ymm2\n\t"
+        "cmp	$32, %3\n\t"
+        "jg	2b\n\t"
+        avx2_chop
+        ...
+#endif
+
+#if 0
+        /*
+         * Will XOP processors have SSSE3/AVX??
+         * And what is the unaligned load performance?
+         */
+        "prefetchnta	0x70(%0)\n\t"
+        "lddqu	(%0), %%xmm0\n\t"
+        "vpaddd	%%xmm3, %%xmm5, %%xmm5\n\t"
+        "sub	$16, %3\n\t"
+        "add	$16, %0\n\t"
+        "cmp	$15, %3\n\t"
+        "vphaddubd	%%xmm0, %%xmm1\n\t" /* A */
+        "vpmaddubsw %%xmm4, %%xmm0, %%xmm0\n\t"/* AVX! */ /* 1 */
+        "vphadduwd	%%xmm0, %%xmm0\n\t" /* 2 */
+        "vpaddd	%%xmm1, %%xmm3, %%xmm3\n\t" /* B: A+B => hadd+acc or vpmadcubd w. mul = 1 */
+        "vpaddd	%%xmm0, %%xmm2, %%xmm2\n\t" /* 3: 1+2+3 => vpmadcubd w. mul = 16,15,14... */
+        "jg	1b\n\t"
+        xop_chop
+        xop_chop
+        xop_chop
+        setup
+        "jg	1b\n\t"
+        "vphaddudq	%%xmm2, %%xmm0\n\t"
+        "vphaddudq	%%xmm3, %%xmm1\n\t"
+        "pshufd	$0xE6, %%xmm0, %%xmm2\n\t"
+        "pshufd	$0xE6, %%xmm1, %%xmm3\n\t"
+        "paddd	%%xmm0, %%xmm2\n\t"
+        "paddd	%%xmm1, %%xmm3\n\t"
+        "movd	%%xmm2, %2\n\t"
+        "movd	%%xmm3, %1\n\t"
+#endif
+
+/* ========================================================================= */
+local uLong adler32_SSSE3(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1 = adler & 0xffff;
+    unsigned int s2 = (adler >> 16) & 0xffff;
+    unsigned int k;
+
+    k    = ALIGN_DIFF(buf, 16);
+    len -= k;
+    if (k)
+        buf = adler32_jumped(buf, &s1, &s2, k);
+
+    __asm__ __volatile__ (
+            "mov	%6, %3\n\t"		/* get max. byte count VNMAX till v1_round_sum overflows */
+            "cmp	%3, %4\n\t"
+            "cmovb	%4, %3\n\t"		/* k = len >= VNMAX ? k : len */
+            "sub	%3, %4\n\t"		/* len -= k */
+            "cmp	$16, %3\n\t"
+            "jb	8f\n\t"			/* if(k < 16) goto OUT */
+#if defined(__ELF__) && !defined(__clang__)
+            ".subsection 2\n\t"
+#else
+            "jmp	7f\n\t"
+#endif
+            ".p2align 2\n"
+            /*
+             * reduction function to bring a vector sum within the range of BASE
+             * This does no full reduction! When the sum is large, a number > BASE
+             * is the result. To do a full reduction call multiple times.
+             */
+            "sse2_chop:\n\t"
+            "movdqa	%%xmm0, %%xmm1\n\t"	/* y = x */
+            "pslld	$16, %%xmm1\n\t"	/* y <<= 16 */
+            "psrld	$16, %%xmm0\n\t"	/* x >>= 16 */
+            "psrld	$16, %%xmm1\n\t"	/* y >>= 16 */
+            "psubd	%%xmm0, %%xmm1\n\t"	/* y -= x */
+            "pslld	$4, %%xmm0\n\t"		/* x <<= 4 */
+            "paddd	%%xmm1, %%xmm0\n\t"	/* x += y */
+            "ret\n\t"
+#if defined(__ELF__) && !defined(__clang__)
+            ".previous\n\t"
+#else
+            "7:\n\t"
+#endif
+            "movdqa	%5, %%xmm5\n\t"		/* get vord_b */
+            "prefetchnta	0x70(%0)\n\t"
+            "movd %2, %%xmm2\n\t"		/* init vector sum vs2 with s2 */
+            "movd %1, %%xmm3\n\t"		/* init vector sum vs1 with s1 */
+            "pxor	%%xmm4, %%xmm4\n"	/* zero */
+            "3:\n\t"
+            "pxor	%%xmm7, %%xmm7\n\t"	/* zero vs1_round_sum */
+            ".p2align 3,,3\n\t"
+            ".p2align 2\n"
+            "2:\n\t"
+            "mov	$128, %1\n\t"		/* inner_k = 128 bytes till vs2_i overflows */
+            "cmp	%1, %3\n\t"
+            "cmovb	%3, %1\n\t"		/* inner_k = k >= inner_k ? inner_k : k */
+            "and	$-16, %1\n\t"		/* inner_k = ROUND_TO(inner_k, 16) */
+            "sub	%1, %3\n\t"		/* k -= inner_k */
+            "shr	$4, %1\n\t"		/* inner_k /= 16 */
+            "pxor	%%xmm6, %%xmm6\n\t"	/* zero vs2_i */
+            ".p2align 4,,7\n"
+            ".p2align 3\n"
+            "1:\n\t"
+            "movdqa	(%0), %%xmm0\n\t"	/* fetch input data */
+            "prefetchnta	0x70(%0)\n\t"
+            "paddd	%%xmm3, %%xmm7\n\t"	/* vs1_round_sum += vs1 */
+            "add	$16, %0\n\t"		/* advance input data pointer */
+            "dec	%1\n\t"			/* decrement inner_k */
+            "movdqa	%%xmm0, %%xmm1\n\t"	/* make a copy of the input data */
+#  if (HAVE_BINUTILS-0) >= 217
+            "pmaddubsw %%xmm5, %%xmm0\n\t"	/* multiply all input bytes by vord_b bytes, add adjecent results to words */
+#  else
+            ".byte 0x66, 0x0f, 0x38, 0x04, 0xc5\n\t" /* pmaddubsw %%xmm5, %%xmm0 */
+#  endif
+            "psadbw	%%xmm4, %%xmm1\n\t"	/* subtract zero from every byte, add 8 bytes to a sum */
+            "paddw	%%xmm0, %%xmm6\n\t"	/* vs2_i += in * vorder_b */
+            "paddd	%%xmm1, %%xmm3\n\t"	/* vs1 += psadbw */
+            "jnz	1b\n\t"			/* repeat if inner_k != 0 */
+            "movdqa	%%xmm6, %%xmm0\n\t"	/* copy vs2_i */
+            "punpckhwd	%%xmm4, %%xmm0\n\t"	/* zero extent vs2_i upper words to dwords */
+            "punpcklwd	%%xmm4, %%xmm6\n\t"	/* zero extent vs2_i lower words to dwords */
+            "paddd	%%xmm0, %%xmm2\n\t"	/* vs2 += vs2_i.upper */
+            "paddd	%%xmm6, %%xmm2\n\t"	/* vs2 += vs2_i.lower */
+            "cmp	$15, %3\n\t"
+            "jg	2b\n\t"				/* if(k > 15) repeat */
+            "movdqa	%%xmm7, %%xmm0\n\t"	/* move vs1_round_sum */
+            "call	sse2_chop\n\t"	/* chop vs1_round_sum */
+            "pslld	$4, %%xmm0\n\t"		/* vs1_round_sum *= 16 */
+            "paddd	%%xmm2, %%xmm0\n\t"	/* vs2 += vs1_round_sum */
+            "call	sse2_chop\n\t"	/* chop again */
+            "movdqa	%%xmm0, %%xmm2\n\t"	/* move vs2 back in place */
+            "movdqa	%%xmm3, %%xmm0\n\t"	/* move vs1 */
+            "call	sse2_chop\n\t"	/* chop */
+            "movdqa	%%xmm0, %%xmm3\n\t"	/* move vs1 back in place */
+            "add	%3, %4\n\t"		/* len += k */
+            "mov	%6, %3\n\t"		/* get max. byte count VNMAX till v1_round_sum overflows */
+            "cmp	%3, %4\n\t"
+            "cmovb	%4, %3\n\t"		/* k = len >= VNMAX ? k : len */
+            "sub	%3, %4\n\t"		/* len -= k */
+            "cmp	$15, %3\n\t"
+            "jg	3b\n\t"				/* if(k > 15) repeat */
+            "pshufd	$0xEE, %%xmm3, %%xmm1\n\t"	/* collect vs1 & vs2 in lowest vector member */
+            "pshufd	$0xEE, %%xmm2, %%xmm0\n\t"
+            "paddd	%%xmm3, %%xmm1\n\t"
+            "paddd	%%xmm2, %%xmm0\n\t"
+            "pshufd	$0xE5, %%xmm0, %%xmm2\n\t"
+            "paddd	%%xmm0, %%xmm2\n\t"
+            "movd	%%xmm1, %1\n\t"		/* mov vs1 to s1 */
+            "movd	%%xmm2, %2\n"		/* mov vs2 to s2 */
+            "8:"
+        : /* %0 */ "=r" (buf),
+          /* %1 */ "=r" (s1),
+          /* %2 */ "=r" (s2),
+          /* %3 */ "=r" (k),
+          /* %4 */ "=r" (len)
+        : /* %5 */ "m" (vord_b),
+          /*
+           * somewhere between 5 & 6, psadbw 64 bit sums ruin the party
+           * spreading the sums with palignr only brings it to 7 (?),
+           * while introducing an op into the main loop (2800 ms -> 3200 ms)
+           */
+          /* %6 */ "i" (5*NMAX),
+          /*    */ "0" (buf),
+          /*    */ "1" (s1),
+          /*    */ "2" (s2),
+          /*    */ "4" (len)
+        : "cc", "memory"
+#  ifdef __SSE__
+          , "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7"
+#  endif
+    );
+
+    if (unlikely(k))
+        buf = adler32_jumped(buf, &s1, &s2, k);
+    MOD28(s1);
+    MOD28(s2);
+    return (s2 << 16) | s1;
+}
+
+/* ========================================================================= */
+local uLong adler32_SSE2(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1 = adler & 0xffff;
+    unsigned int s2 = (adler >> 16) & 0xffff;
+    unsigned int k;
+
+    k    = ALIGN_DIFF(buf, 16);
+    len -= k;
+    if (k)
+        buf = adler32_jumped(buf, &s1, &s2, k);
+
+    __asm__ __volatile__ (
+            "mov	%6, %3\n\t"
+            "cmp	%3, %4\n\t"
+            "cmovb	%4, %3\n\t"
+            "sub	%3, %4\n\t"
+            "cmp	$16, %3\n\t"
+            "jb	8f\n\t"
+            "prefetchnta	0x70(%0)\n\t"
+            "movd	%1, %%xmm4\n\t"
+            "movd	%2, %%xmm3\n\t"
+            "pxor	%%xmm2, %%xmm2\n\t"
+            "pxor	%%xmm5, %%xmm5\n\t"
+            ".p2align 2\n"
+            "3:\n\t"
+            "pxor	%%xmm6, %%xmm6\n\t"
+            "pxor	%%xmm7, %%xmm7\n\t"
+            "mov	$2048, %1\n\t"		/* get byte count till vs2_{l|h}_word overflows */
+            "cmp	%1, %3\n\t"
+            "cmovb	%3, %1\n"
+            "and	$-16, %1\n\t"
+            "sub	%1, %3\n\t"
+            "shr	$4, %1\n\t"
+            ".p2align 4,,7\n"
+            ".p2align 3\n"
+            "1:\n\t"
+            "prefetchnta	0x70(%0)\n\t"
+            "movdqa	(%0), %%xmm0\n\t"	/* fetch input data */
+            "paddd	%%xmm4, %%xmm5\n\t"	/* vs1_round_sum += vs1 */
+            "add	$16, %0\n\t"
+            "dec	%1\n\t"
+            "movdqa	%%xmm0, %%xmm1\n\t"	/* copy input data */
+            "psadbw	%%xmm2, %%xmm0\n\t"	/* add all bytes horiz. */
+            "paddd	%%xmm0, %%xmm4\n\t"	/* add that to vs1 */
+            "movdqa	%%xmm1, %%xmm0\n\t"	/* copy input data */
+            "punpckhbw	%%xmm2, %%xmm1\n\t"	/* zero extent input upper bytes to words */
+            "punpcklbw	%%xmm2, %%xmm0\n\t"	/* zero extent input lower bytes to words */
+            "paddw	%%xmm1, %%xmm7\n\t"	/* vs2_h_words += in_high_words */
+            "paddw	%%xmm0, %%xmm6\n\t"	/* vs2_l_words += in_low_words */
+            "jnz	1b\n\t"
+            "cmp	$15, %3\n\t"
+            "pmaddwd	32+%5, %%xmm7\n\t"	/* multiply vs2_h_words with order, add adjecend results */
+            "pmaddwd	16+%5, %%xmm6\n\t"	/* multiply vs2_l_words with order, add adjecend results */
+            "paddd	%%xmm7, %%xmm3\n\t"	/* add to vs2 */
+            "paddd	%%xmm6, %%xmm3\n\t"	/* add to vs2 */
+            "jg	3b\n\t"
+            "movdqa	%%xmm5, %%xmm0\n\t"
+            "pxor	%%xmm5, %%xmm5\n\t"
+            "call	sse2_chop\n\t"
+            "pslld	$4, %%xmm0\n\t"
+            "paddd	%%xmm3, %%xmm0\n\t"
+            "call	sse2_chop\n\t"
+            "movdqa	%%xmm0, %%xmm3\n\t"
+            "movdqa	%%xmm4, %%xmm0\n\t"
+            "call	sse2_chop\n\t"
+            "movdqa	%%xmm0, %%xmm4\n\t"
+            "add	%3, %4\n\t"
+            "mov	%6, %3\n\t"
+            "cmp	%3, %4\n\t"
+            "cmovb	%4, %3\n"
+            "sub	%3, %4\n\t"
+            "cmp	$15, %3\n\t"
+            "jg	3b\n\t"
+            "pshufd	$0xEE, %%xmm4, %%xmm1\n\t"
+            "pshufd	$0xEE, %%xmm3, %%xmm0\n\t"
+            "paddd	%%xmm4, %%xmm1\n\t"
+            "paddd	%%xmm3, %%xmm0\n\t"
+            "pshufd	$0xE5, %%xmm0, %%xmm3\n\t"
+            "paddd	%%xmm0, %%xmm3\n\t"
+            "movd	%%xmm1, %1\n\t"
+            "movd	%%xmm3, %2\n"
+            "8:\n\t"
+        : /* %0 */ "=r" (buf),
+          /* %1 */ "=r" (s1),
+          /* %2 */ "=r" (s2),
+          /* %3 */ "=r" (k),
+          /* %4 */ "=r" (len)
+        : /* %5 */ "m" (vord),
+          /* %6 */ "i" (5*NMAX),
+          /*    */ "0" (buf),
+          /*    */ "1" (s1),
+          /*    */ "2" (s2),
+          /*    */ "3" (k),
+          /*    */ "4" (len)
+        : "cc", "memory"
+#  ifdef __SSE__
+          , "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7"
+#  endif
+    );
+
+    if (unlikely(k))
+        buf = adler32_jumped(buf, &s1, &s2, k);
+    MOD28(s1);
+    MOD28(s2);
+    return (s2 << 16) | s1;
+}
+
+#  if 0
+/* ========================================================================= */
+/*
+ * The SSE2 version above is faster on my CPUs (Athlon64, Core2,
+ * P4 Xeon, K10 Sempron), but has instruction stalls only a
+ * Out-Of-Order-Execution CPU can solve.
+ * So this Version _may_ be better for the new old thing, Atom.
+ */
+local noinline uLong adler32_SSE2_no_oooe(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1 = adler & 0xffff;
+    unsigned int s2 = (adler >> 16) & 0xffff;
+    unsigned int k;
+
+    k    = ALIGN_DIFF(buf, 16);
+    len -= k;
+    if (k)
+        buf = adler32_jumped(buf, &s1, &s2, k);
+
+    __asm__ __volatile__ (
+            "mov	%6, %3\n\t"
+            "cmp	%3, %4\n\t"
+            "cmovb	%4, %3\n\t"
+            "sub	%3, %4\n\t"
+            "cmp	$16, %3\n\t"
+            "jb	8f\n\t"
+            "movdqa	16+%5, %%xmm6\n\t"
+            "movdqa	32+%5, %%xmm5\n\t"
+            "prefetchnta	16(%0)\n\t"
+            "pxor	%%xmm7, %%xmm7\n\t"
+            "movd	%1, %%xmm4\n\t"
+            "movd	%2, %%xmm3\n\t"
+            ".p2align 3,,3\n\t"
+            ".p2align 2\n"
+            "1:\n\t"
+            "prefetchnta	32(%0)\n\t"
+            "movdqa	(%0), %%xmm1\n\t"
+            "sub	$16, %3\n\t"
+            "movdqa	%%xmm4, %%xmm2\n\t"
+            "add	$16, %0\n\t"
+            "movdqa	%%xmm1, %%xmm0\n\t"
+            "cmp	$15, %3\n\t"
+            "pslld	$4, %%xmm2\n\t"
+            "paddd	%%xmm3, %%xmm2\n\t"
+            "psadbw	%%xmm7, %%xmm0\n\t"
+            "paddd	%%xmm0, %%xmm4\n\t"
+            "movdqa	%%xmm1, %%xmm0\n\t"
+            "punpckhbw	%%xmm7, %%xmm1\n\t"
+            "punpcklbw	%%xmm7, %%xmm0\n\t"
+            "movdqa	%%xmm1, %%xmm3\n\t"
+            "pmaddwd	%%xmm6, %%xmm0\n\t"
+            "paddd	%%xmm2, %%xmm0\n\t"
+            "pmaddwd	%%xmm5, %%xmm3\n\t"
+            "paddd	%%xmm0, %%xmm3\n\t"
+            "jg	1b\n\t"
+            "movdqa	%%xmm3, %%xmm0\n\t"
+            "call	sse2_chop\n\t"
+            "call	sse2_chop\n\t"
+            "movdqa	%%xmm0, %%xmm3\n\t"
+            "movdqa	%%xmm4, %%xmm0\n\t"
+            "call	sse2_chop\n\t"
+            "movdqa	%%xmm0, %%xmm4\n\t"
+            "add	%3, %4\n\t"
+            "mov	%6, %3\n\t"
+            "cmp	%3, %4\n\t"
+            "cmovb	%4, %3\n\t"
+            "sub	%3, %4\n\t"
+            "cmp	$15, %3\n\t"
+            "jg	1b\n\t"
+            "pshufd	$0xEE, %%xmm3, %%xmm0\n\t"
+            "pshufd	$0xEE, %%xmm4, %%xmm1\n\t"
+            "paddd	%%xmm3, %%xmm0\n\t"
+            "pshufd	$0xE5, %%xmm0, %%xmm2\n\t"
+            "paddd	%%xmm4, %%xmm1\n\t"
+            "movd	%%xmm1, %1\n\t"
+            "paddd	%%xmm0, %%xmm2\n\t"
+            "movd	%%xmm2, %2\n"
+            "8:"
+        : /* %0 */ "=r" (buf),
+          /* %1 */ "=r" (s1),
+          /* %2 */ "=r" (s2),
+          /* %3 */ "=r" (k),
+          /* %4 */ "=r" (len)
+        : /* %5 */ "m" (vord),
+          /* %6 */ "i" (NMAX + NMAX/3),
+          /*    */ "0" (buf),
+          /*    */ "1" (s1),
+          /*    */ "2" (s2),
+          /*    */ "4" (len)
+        : "cc", "memory"
+#  ifdef __SSE__
+          , "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7"
+#  endif
+    );
+
+    if (unlikely(k))
+        buf = adler32_jumped(buf, &s1, &s2, k);
+    MOD28(s1);
+    MOD28(s2);
+    return (s2 << 16) | s1;
+}
+#  endif
+
+#  ifndef __x86_64__
+/* ========================================================================= */
+/*
+ * SSE version to help VIA-C3_2, P2 & P3
+ */
+local uLong adler32_SSE(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1 = adler & 0xffff;
+    unsigned int s2 = (adler >> 16) & 0xffff;
+    unsigned int k;
+
+    k    = ALIGN_DIFF(buf, 8);
+    len -= k;
+    if (k)
+        buf = adler32_jumped(buf, &s1, &s2, k);
+
+    __asm__ __volatile__ (
+            "mov	%6, %3\n\t"
+            "cmp	%3, %4\n\t"
+            "cmovb	%4, %3\n\t"
+            "sub	%3, %4\n\t"
+            "cmp	$8, %3\n\t"
+            "jb	8f\n\t"
+            "movd	%1, %%mm4\n\t"
+            "movd	%2, %%mm3\n\t"
+            "pxor	%%mm2, %%mm2\n\t"
+            "pxor	%%mm5, %%mm5\n\t"
+#    ifdef __ELF__
+            ".subsection 2\n\t"
+#    else
+            "jmp	7f\n\t"
+#    endif
+            ".p2align 2\n"
+            "mmx_chop:\n\t"
+            "movq	%%mm0, %%mm1\n\t"
+            "pslld	$16, %%mm1\n\t"
+            "psrld	$16, %%mm0\n\t"
+            "psrld	$16, %%mm1\n\t"
+            "psubd	%%mm0, %%mm1\n\t"
+            "pslld	$4, %%mm0\n\t"
+            "paddd	%%mm1, %%mm0\n\t"
+            "ret\n\t"
+#    ifdef __ELF__
+            ".previous\n\t"
+#    else
+            "7:\n\t"
+#    endif
+            ".p2align	2\n"
+            "3:\n\t"
+            "pxor	%%mm6, %%mm6\n\t"
+            "pxor	%%mm7, %%mm7\n\t"
+            "mov	$1024, %1\n\t"
+            "cmp	%1, %3\n\t"
+            "cmovb	%3, %1\n"
+            "and	$-8, %1\n\t"
+            "sub	%1, %3\n\t"
+            "shr	$3, %1\n\t"
+            ".p2align 4,,7\n"
+            ".p2align 3\n"
+            "1:\n\t"
+            "movq	(%0), %%mm0\n\t"
+            "paddd	%%mm4, %%mm5\n\t"
+            "add	$8, %0\n\t"
+            "dec	%1\n\t"
+            "movq	%%mm0, %%mm1\n\t"
+            "psadbw	%%mm2, %%mm0\n\t"
+            "paddd	%%mm0, %%mm4\n\t"
+            "movq	%%mm1, %%mm0\n\t"
+            "punpckhbw	%%mm2, %%mm1\n\t"
+            "punpcklbw	%%mm2, %%mm0\n\t"
+            "paddw	%%mm1, %%mm7\n\t"
+            "paddw	%%mm0, %%mm6\n\t"
+            "jnz	1b\n\t"
+            "cmp	$7, %3\n\t"
+            "pmaddwd	40+%5, %%mm7\n\t"
+            "pmaddwd	32+%5, %%mm6\n\t"
+            "paddd	%%mm7, %%mm3\n\t"
+            "paddd	%%mm6, %%mm3\n\t"
+            "jg	3b\n\t"
+            "movq	%%mm5, %%mm0\n\t"
+            "pxor	%%mm5, %%mm5\n\t"
+            "call	mmx_chop\n\t"
+            "pslld	$3, %%mm0\n\t"
+            "paddd	%%mm3, %%mm0\n\t"
+            "call	mmx_chop\n\t"
+            "movq	%%mm0, %%mm3\n\t"
+            "movq	%%mm4, %%mm0\n\t"
+            "call	mmx_chop\n\t"
+            "movq	%%mm0, %%mm4\n\t"
+            "add	%3, %4\n\t"
+            "mov	%6, %3\n\t"
+            "cmp	%3, %4\n\t"
+            "cmovb	%4, %3\n"
+            "sub	%3, %4\n\t"
+            "cmp	$7, %3\n\t"
+            "jg	3b\n\t"
+            "movd	%%mm4, %1\n\t"
+            "psrlq	$32, %%mm4\n\t"
+            "movd	%%mm3, %2\n\t"
+            "psrlq	$32, %%mm3\n\t"
+            "movd	%%mm4, %4\n\t"
+            "add	%4, %1\n\t"
+            "movd	%%mm3, %4\n\t"
+            "add	%4, %2\n\t"
+            "emms\n"
+            "8:\n\t"
+        : /* %0 */ "=r" (buf),
+          /* %1 */ "=r" (s1),
+          /* %2 */ "=r" (s2),
+          /* %3 */ "=r" (k),
+          /* %4 */ "=r" (len)
+        : /* %5 */ "m" (vord),
+          /* %6 */ "i" ((5*NMAX)/2),
+          /*    */ "0" (buf),
+          /*    */ "1" (s1),
+          /*    */ "2" (s2),
+          /*    */ "3" (k),
+          /*    */ "4" (len)
+        : "cc", "memory"
+#    ifdef __MMX__
+          , "mm0", "mm1", "mm2", "mm3", "mm4", "mm5", "mm6", "mm7"
+#    endif
+                               );
+
+    if (unlikely(k))
+        buf = adler32_jumped(buf, &s1, &s2, k);
+    MOD28(s1);
+    MOD28(s2);
+    return (s2 << 16) | s1;
+}
+
+/* ========================================================================= */
+/*
+ * Processors which only have MMX will prop. not like this
+ * code, they are so old, they are not Out-Of-Order
+ * (maybe except AMD K6, Cyrix, Winchip/VIA).
+ * I did my best to get at least 1 instruction between result -> use
+ */
+local uLong adler32_MMX(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned int s1 = adler & 0xffff;
+    unsigned int s2 = (adler >> 16) & 0xffff;
+    unsigned int k;
+
+    k    = ALIGN_DIFF(buf, 8);
+    len -= k;
+    if (k)
+        buf = adler32_jumped(buf, &s1, &s2, k);
+
+    __asm__ __volatile__ (
+            "mov	%6, %3\n\t"
+            "cmp	%3, %4\n\t"
+            "jae	6f\n\t"
+            "mov	%4, %3\n"
+            "6:\n\t"
+            "sub	%3, %4\n\t"
+            "cmp	$8, %3\n\t"
+            "jb	8f\n\t"
+            "sub	$8, %%esp\n\t"
+            "movd	%1, %%mm4\n\t"
+            "movd	%2, %%mm2\n\t"
+            "movq	%5, %%mm3\n"
+            "5:\n\t"
+            "movq	%%mm2, %%mm0\n\t"
+            "pxor	%%mm2, %%mm2\n\t"
+            "pxor	%%mm5, %%mm5\n\t"
+            ".p2align 2\n"
+            "3:\n\t"
+            "movq	%%mm0, (%%esp)\n\t"
+            "pxor	%%mm6, %%mm6\n\t"
+            "pxor	%%mm7, %%mm7\n\t"
+            "mov	$1024, %1\n\t"
+            "cmp	%1, %3\n\t"
+            "jae	4f\n\t"
+            "mov	%3, %1\n"
+            "4:\n\t"
+            "and	$-8, %1\n\t"
+            "sub	%1, %3\n\t"
+            "shr	$3, %1\n\t"
+            ".p2align 4,,7\n\t"
+            ".p2align 3\n"
+            "1:\n\t"
+            "movq	(%0), %%mm0\n\t"
+            "paddd	%%mm4, %%mm5\n\t"
+            "add	$8, %0\n\t"
+            "dec	%1\n\t"
+            "movq	%%mm0, %%mm1\n\t"
+            "punpcklbw	%%mm2, %%mm0\n\t"
+            "punpckhbw	%%mm2, %%mm1\n\t"
+            "paddw	%%mm0, %%mm6\n\t"
+            "paddw	%%mm1, %%mm0\n\t"
+            "paddw	%%mm1, %%mm7\n\t"
+            "pmaddwd	%%mm3, %%mm0\n\t"
+            "paddd	%%mm0, %%mm4\n\t"
+            "jnz	1b\n\t"
+            "movq	(%%esp), %%mm0\n\t"
+            "cmp	$7, %3\n\t"
+            "pmaddwd	32+%5, %%mm6\n\t"
+            "pmaddwd	40+%5, %%mm7\n\t"
+            "paddd	%%mm6, %%mm0\n\t"
+            "paddd	%%mm7, %%mm0\n\t"
+            "jg	3b\n\t"
+            "movq	%%mm0, %%mm2\n\t"
+            "movq	%%mm5, %%mm0\n\t"
+            "call	mmx_chop\n\t"
+            "pslld	$3, %%mm0\n\t"
+            "paddd	%%mm2, %%mm0\n\t"
+            "call	mmx_chop\n\t"
+            "movq	%%mm0, %%mm2\n\t"
+            "movq	%%mm4, %%mm0\n\t"
+            "call	mmx_chop\n\t"
+            "movq	%%mm0, %%mm4\n\t"
+            "add	%3, %4\n\t"
+            "mov	%6, %3\n\t"
+            "cmp	%3, %4\n\t"
+            "jae	2f\n\t"
+            "mov	%4, %3\n"
+            "2:\n\t"
+            "sub	%3, %4\n\t"
+            "cmp	$7, %3\n\t"
+            "jg	5b\n\t"
+            "add	$8, %%esp\n\t"
+            "movd	%%mm4, %1\n\t"
+            "psrlq	$32, %%mm4\n\t"
+            "movd	%%mm2, %2\n\t"
+            "psrlq	$32, %%mm2\n\t"
+            "movd	%%mm4, %4\n\t"
+            "add	%4, %1\n\t"
+            "movd	%%mm2, %4\n\t"
+            "add	%4, %2\n\t"
+            "emms\n"
+            "8:\n\t"
+        : /* %0 */ "=r" (buf),
+          /* %1 */ "=r" (s1),
+          /* %2 */ "=r" (s2),
+          /* %3 */ "=r" (k),
+          /* %4 */ "=r" (len)
+        : /* %5 */ "m" (vord),
+          /* %6 */ "i" (4*NMAX),
+          /*    */ "0" (buf),
+          /*    */ "1" (s1),
+          /*    */ "2" (s2),
+          /*    */ "3" (k),
+          /*    */ "4" (len)
+        : "cc", "memory"
+#    ifdef __MMX__
+          , "mm0", "mm1", "mm2", "mm3", "mm4", "mm5", "mm6", "mm7"
+#    endif
+    );
+
+    if (unlikely(k))
+        buf = adler32_jumped(buf, &s1, &s2, k);
+    MOD28(s1);
+    MOD28(s2);
+    return (s2 << 16) | s1;
+}
+#  endif
+
+/* ========================================================================= */
+local uLong adler32_x86(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    /* split Adler-32 into component sums */
+    unsigned int s1 = adler & 0xffff;
+    unsigned int s2 = (adler >> 16) & 0xffff;
+    unsigned int n;
+
+    do {
+        /* find maximum, len or NMAX */
+        n = len < NMAX ? len : NMAX;
+        len -= n;
+
+        /* do it */
+        buf = adler32_jumped(buf, &s1, &s2, n);
+        /* modulo */
+        MOD(s1);
+        MOD(s2);
+    } while (likely(len));
+
+    /* return recombined sums */
+    return (s2 << 16) | s1;
+}
+
+/* ========================================================================= */
+#define NO_ADLER32_GE16
+local noinline uLong adler32_ge16(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    /* split Adler-32 into component sums */
+    unsigned int s1 = adler & 0xffff;
+    unsigned int s2 = (adler >> 16) & 0xffff;
+
+    /* simply do it, we do not expect more then NMAX as len */
+    adler32_jumped(buf, &s1, &s2, len);
+    /* actually we expect much less, MOD28 it */
+    MOD28(s1);
+    MOD28(s2);
+
+    /* return recombined sums */
+    return (s2 << 16) | s1;
+}
+
+/* ========================================================================= */
+/*
+ * Knot it all together with a runtime switch
+ */
+
+/* Flags */
+#  define CFF_DEFAULT (1 << 0)
+/* Processor features */
+#  define CFEATURE_CMOV  (15 +   0)
+#  define CFEATURE_MMX   (23 +   0)
+#  define CFEATURE_SSE   (25 +   0)
+#  define CFEATURE_SSE2  (26 +   0)
+#  define CFEATURE_SSSE3 ( 9 +  32)
+
+#  define CFB(x) (1 << ((x)%32))
+
+#  define FEATURE_WORDS 2
+
+/* ========================================================================= */
+/* data structure */
+struct test_cpu_feature
+{
+    int f_type;
+    int flags;
+    unsigned int features[FEATURE_WORDS];
+};
+
+/* ========================================================================= */
+/* function enum */
+enum adler32_types
+{
+    T_ADLER32_RTSWITCH = 0,
+    T_ADLER32_X86,
+#  ifndef __x86_64__
+    T_ADLER32_MMX,
+    T_ADLER32_SSE,
+#  endif
+    T_ADLER32_SSE2,
+    T_ADLER32_SSSE3,
+    T_ADLER32_MAX
+};
+
+/* ========================================================================= */
+/*
+ * Decision table
+ */
+
+local const struct test_cpu_feature tfeat_adler32_vec[] =
+{
+    /* func                             flags   features       */
+    {T_ADLER32_SSSE3,         0, {CFB(CFEATURE_CMOV), CFB(CFEATURE_SSSE3)}},
+    {T_ADLER32_SSE2,          0, {CFB(CFEATURE_SSE2)|CFB(CFEATURE_CMOV), 0}},
+#  ifndef __x86_64__
+    {T_ADLER32_SSE,           0, {CFB(CFEATURE_SSE)|CFB(CFEATURE_CMOV), 0}},
+    {T_ADLER32_MMX,           0, {CFB(CFEATURE_MMX), 0}},
+#  endif
+    {T_ADLER32_X86, CFF_DEFAULT, { 0, 0}},
+};
+
+/* ========================================================================= */
+/* Prototypes */
+local noinline int test_cpu_feature(const struct test_cpu_feature *t, unsigned int l);
+local uLong adler32_vec_runtimesw(uLong adler, const Bytef *buf, uInt len);
+
+/* ========================================================================= */
+/*
+ * Function pointer table
+ */
+local uLong (*const adler32_ptr_tab[])(uLong adler, const Bytef *buf, uInt len) =
+{
+    adler32_vec_runtimesw,
+    adler32_x86,
+#  ifndef __x86_64__
+    adler32_MMX,
+    adler32_SSE,
+#  endif
+    adler32_SSE2,
+    adler32_SSSE3,
+};
+
+/* ========================================================================= */
+#  if _FORTIFY_SOURCE-0 > 0
+/*
+ * Runtime decide var
+ */
+local enum adler32_types adler32_f_type = T_ADLER32_RTSWITCH;
+#  else
+/*
+ * Runtime Function pointer
+ */
+local uLong (*adler32_vec_ptr)(uLong adler, const Bytef *buf, uInt len) = adler32_vec_runtimesw;
+#  endif
+
+/* ========================================================================= */
+/*
+ * Constructor to init the decide var early
+ */
+local GCC_ATTR_CONSTRUCTOR void adler32_vec_select(void)
+{
+    enum adler32_types lf_type =
+        test_cpu_feature(tfeat_adler32_vec, sizeof (tfeat_adler32_vec)/sizeof (tfeat_adler32_vec[0]));
+#  if _FORTIFY_SOURCE-0 > 0
+    adler32_f_type = lf_type;
+#  else
+    adler32_vec_ptr = adler32_ptr_tab[lf_type];
+#  endif
+}
+
+/* ========================================================================= */
+/*
+ * Jump function
+ */
+local noinline uLong adler32_vec(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    /*
+     * Protect us from memory corruption. As long as the function pointer table
+     * resides in rodata, with a little bounding we can prevent arb. code
+     * execution (overwritten vtable pointer). We still may crash if the corruption
+     * is within bounds (or the cpudata gets corrupted too) and we jump into an
+     * function with unsupported instr., but this should mitigate the worst case
+     * scenario.
+     * But it's more expensive than a simple function pointer, so only when more
+     * security is wanted.
+     */
+#  if _FORTIFY_SOURCE-0 > 0
+    enum adler32_types lf_type = adler32_f_type;
+    /*
+     * If the compiler is smart he creates a cmp + sbb + and, cmov have a high
+     * latency and are not always avail.
+     * Otherwise compiler logic is advanced enough to see what's happening here,
+     * so there maybe is a reason why he changes this to a cmov...
+     * (or he simply does not see he can create a conditional -1/0 the cheap way)
+     *
+     * Maybe change it to an unlikely() cbranch? Which still leaves the question
+     * what's the mispredition propability, esp. with lots of different x86
+     * microarchs and not always perfect CFLAGS (-march/-mtune) to arrange the
+     * code to the processors liking.
+     */
+    lf_type &= likely((unsigned)lf_type < (unsigned)T_ADLER32_MAX) ? -1 : 0;
+    return adler32_ptr_tab[lf_type](adler, buf, len);
+#  else
+    return adler32_vec_ptr(adler, buf, len);
+#  endif
+}
+
+/* ========================================================================= */
+/*
+ * the runtime switcher is a little racy, but this is OK,
+ * it should normaly not run if the constructor works, and
+ * we are on x86, which isn't that picky about ordering
+ */
+local uLong adler32_vec_runtimesw(uLong adler, const Bytef *buf, uInt len)
+{
+    adler32_vec_select();
+    return adler32_vec(adler, buf, len);
+}
+
+/* ========================================================================= */
+/* Internal data types */
+struct cpuid_regs
+{
+    unsigned long eax, ebx, ecx, edx;
+};
+
+local struct
+{
+    unsigned int max_basic;
+    unsigned int features[FEATURE_WORDS];
+    int init_done;
+} our_cpu;
+
+/* ========================================================================= */
+local inline unsigned long read_flags(void)
+{
+    unsigned long f;
+    __asm__ __volatile__ (
+            "pushf\n\t"
+            "pop %0\n\t"
+        : "=r" (f)
+    );
+    return f;
+}
+
+/* ========================================================================= */
+local inline void write_flags(unsigned long f)
+{
+    __asm__ __volatile__ (
+            "push %0\n\t"
+            "popf\n\t"
+        : : "ri" (f) : "cc"
+    );
+}
+
+/* ========================================================================= */
+local inline void cpuid(struct cpuid_regs *regs, unsigned long func)
+{
+    /* save ebx around cpuid call, PIC code needs it */
+    __asm__ __volatile__ (
+            "xchg	%1, " PICREG "\n\t"
+            "cpuid\n\t"
+            "xchg	%1, " PICREG "\n"
+        : /* %0 */ "=a" (regs->eax),
+          /* %1 */ "=r" (regs->ebx),
+          /* %2 */ "=c" (regs->ecx),
+          /* %4 */ "=d" (regs->edx)
+        : /* %5 */ "0" (func),
+          /* %6 */ "2" (regs->ecx)
+        : "cc"
+    );
+}
+
+/* ========================================================================= */
+local inline void cpuids(struct cpuid_regs *regs, unsigned long func)
+{
+    regs->ecx = 0;
+    cpuid(regs, func);
+}
+
+/* ========================================================================= */
+local inline int toggle_eflags_test(const unsigned long mask)
+{
+    unsigned long f;
+    int result;
+
+    f = read_flags();
+    write_flags(f ^ mask);
+    result = !!((f ^ read_flags()) & mask);
+    /*
+     * restore the old flags, the test for i486 tests the alignment
+     * check bit, and left set will confuse the x86 software world.
+     */
+    write_flags(f);
+    return result;
+}
+
+/* ========================================================================= */
+local inline int is_486(void)
+{
+    return toggle_eflags_test(1 << 18);
+}
+
+/* ========================================================================= */
+local inline int has_cpuid(void)
+{
+    return toggle_eflags_test(1 << 21);
+}
+
+/* ========================================================================= */
+local void identify_cpu(void)
+{
+    struct cpuid_regs a;
+
+    if (our_cpu.init_done)
+        return;
+
+    our_cpu.init_done = -1;
+    /* force a write out to memory */
+    __asm__ __volatile__ ("" : : "m" (our_cpu.init_done));
+
+    if (!is_486())
+        return;
+
+    if (!has_cpuid())
+        return;
+
+    /* get the maximum basic leaf number */
+    cpuids(&a, 0x00000000);
+    our_cpu.max_basic = (unsigned int)a.eax;
+    /* we could get the vendor string from ebx, edx, ecx */
+
+    /* get the first basic leaf, if it is avail. */
+    if (our_cpu.max_basic >= 0x00000001)
+        cpuids(&a, 0x00000001);
+    else
+        a.eax = a.ebx = a.ecx = a.edx = 0;
+
+    /* we could extract family, model, stepping from eax */
+
+    /* there is the first set of features */
+    our_cpu.features[0] = a.edx;
+    our_cpu.features[1] = a.ecx;
+
+    /* now we could test the extended features, but is not needed, for now */
+}
+
+/* ========================================================================= */
+local noinline int test_cpu_feature(const struct test_cpu_feature *t, unsigned int l)
+{
+    unsigned int i, j, f;
+    identify_cpu();
+
+    for (i = 0; i < l; i++) {
+        if (t[i].flags & CFF_DEFAULT)
+            return t[i].f_type;
+        for (f = 0, j = 0; j < FEATURE_WORDS; j++)
+            f |= (our_cpu.features[j] & t[i].features[j]) ^ t[i].features[j];
+        if (f)
+            continue;
+        return t[i].f_type;
+    }
+    return 1; /* default */
+}
+
+#endif
diff --git a/zutil.h b/zutil.h
index 4e3dcc6..bed2509 100644
--- a/zutil.h
+++ b/zutil.h
@@ -13,6 +13,8 @@
 #ifndef ZUTIL_H
 #define ZUTIL_H
 
+#define GCC_VERSION_GE(x) ((__GNUC__-0) * 100 + __GNUC_MINOR__-0 >= x)
+
 #ifdef HAVE_HIDDEN
 #  define ZLIB_INTERNAL __attribute__((visibility ("hidden")))
 #else
@@ -21,6 +23,36 @@
 
 #include "zlib.h"
 
+#if GCC_VERSION_GE(301)
+/* sometimes leaks out of old kernel header */
+#  undef noinline
+#  define noinline __attribute__((__noinline__))
+#else
+#  ifndef noinline
+#    define noinline
+#  endif
+#endif
+
+#if GCC_VERSION_GE(301)
+#  define GCC_ATTR_UNUSED_PARAM __attribute__((__unused__))
+#else
+#  define GCC_ATTR_UNUSED_PARAM
+#endif
+
+#if GCC_VERSION_GE(296)
+#  undef likely
+#  undef unlikely
+#  define likely(x)   __builtin_expect(!!(x), 1)
+#  define unlikely(x) __builtin_expect(!!(x), 0)
+#else
+#  ifndef likely
+#    define likely(x)   (x)
+#  endif
+#  ifndef unlikely
+#    define unlikely(x) (x)
+#  endif
+#endif
+
 #if defined(STDC) && !defined(Z_SOLO)
 #  if !(defined(_WIN32_WCE) && defined(_MSC_VER))
 #    include <stddef.h>
@@ -33,6 +65,12 @@
    typedef long ptrdiff_t;  /* guess -- will be caught if guess is wrong */
 #endif
 
+#define ROUND_TO(x , n) ((x) & ~((n) - 1L))
+#define DIV_ROUNDUP(a, b) (((a) + (b) - 1) / (b))
+#define ALIGN_DIFF(x, n) ((((intptr_t)((x)+(n) - 1L) & ~((intptr_t)(n) - 1L))) - (intptr_t)(x))
+#define ALIGN_DOWN(x, n) (((intptr_t)(x)) & ~((intptr_t)(n) - 1L))
+#define ALIGN_DOWN_DIFF(x, n) (((intptr_t)(x)) & ((intptr_t)(n) - 1L))
+
 #ifndef local
 #  define local static
 #endif
@@ -161,6 +199,14 @@ extern const char * const z_errmsg[10]; /* indexed by 2-zlib_error */
 #  endif
 #endif
 
+#ifndef UINT64_C
+#  if defined(_MSC_VER) || defined(__BORLANDC__)
+#    define UINT64_C(c)    (c ## ui64)
+#  else
+#    define UINT64_C(c)    (c ## ULL)
+#  endif
+#endif
+
 #if defined(__BORLANDC__) && !defined(MSDOS)
   #pragma warn -8004
   #pragma warn -8008
